{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_extraction import *\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack,csr_matrix\n",
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicative_keyword(train_df_,test_df_,col,global_prob,minFreq=10,q=0.975,verbose=False):\n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    train_df[col] = train_df[col].map(lambda x:re.sub('\\W',' ',x))\n",
    "    train_df[col] = train_df[col].map(lambda x:' '.join(x.split()))\n",
    "    test_df[col] = test_df[col].map(lambda x:re.sub('\\W',' ',x))\n",
    "    test_df[col] = test_df[col].map(lambda x:' '.join(x.split()))\n",
    "\n",
    "    train_df['low'] = train_df['interest_level'].map(lambda x:int(x==0))\n",
    "    train_df['medium'] = train_df['interest_level'].map(lambda x:int(x==1))\n",
    "    train_df['high'] = train_df['interest_level'].map(lambda x:int(x==2))\n",
    "    \n",
    "    count_vect = CountVectorizer(ngram_range=(1,2),stop_words=None,min_df = minFreq)\n",
    "    count_vect.fit(train_df[col])\n",
    "\n",
    "    allpairs = {'key':[],'kl distance':[],'low':[],'medium':[],'high':[],'count':[]}\n",
    "    for key,_ in count_vect.vocabulary_.iteritems():\n",
    "        tmpdf = train_df[train_df[col].map(lambda x:key in x)]\n",
    "        if len(tmpdf)<minFreq:\n",
    "            if verbose:\n",
    "                print 'cannot find {}'.format(key)\n",
    "            continue\n",
    "        p = np.array(tmpdf[['low','medium','high']].sum())*1./len(tmpdf)\n",
    "\n",
    "        #similarity = np.sum(p*global_prob)\n",
    "        kl_distance = entropy(p,global_prob)\n",
    "        tmpdf_test = test_df[test_df[col].map(lambda x:key in x)]\n",
    "\n",
    "        #if similarity < 0.3 and len(tmpdf_test)>minFreq:\n",
    "        #    print 'key={}, length in test={}'.format(key,len(tmpdf_test))\n",
    "        #    print 'kl distance={}'.format(kl_distance)\n",
    "        #    print p\n",
    "        if len(tmpdf_test)>minFreq and len(tmpdf)>minFreq:\n",
    "            allpairs['key'].append(key)\n",
    "            #allpairs['similarity'].append(similarity)\n",
    "            allpairs['kl distance'].append(kl_distance)\n",
    "            allpairs['low'].append(p[0])\n",
    "            allpairs['medium'].append(p[1])\n",
    "            allpairs['high'].append(p[2])\n",
    "            allpairs['count'].append(len(tmpdf))\n",
    "    allpairs = pd.DataFrame(allpairs)\n",
    "    kl_quant = allpairs['kl distance'].quantile(q)\n",
    "    allpairs = allpairs[allpairs['kl distance']>kl_quant]\n",
    "    \n",
    "    count_vect = CountVectorizer(ngram_range=(1,2),stop_words=None,vocabulary=list(allpairs['key']))\n",
    "    trainX = count_vect.transform(train_df[col])\n",
    "    #import pdb;pdb.set_trace()\n",
    "    testX = count_vect.transform(test_df[col])\n",
    "    return trainX,testX,allpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_fea(df,combined_fea,minFreq):    \n",
    "    fea_list = []        \n",
    "    all_fea = defaultdict(int)\n",
    "    for _,row in df.iterrows():\n",
    "        tmp = []\n",
    "        for xx in row['features']:        \n",
    "            theword = xx\n",
    "            for kk,vv in combined_fea.iteritems():\n",
    "                if xx in vv:\n",
    "                    theword == kk\n",
    "                    break\n",
    "            tmp.append(theword)\n",
    "            all_fea[theword] += 1\n",
    "        fea_list.append(tmp)\n",
    "    if len(fea_list)!=len(df):\n",
    "        raise ValueError(\"length of two data do not match\")\n",
    "    df['features'] = fea_list\n",
    "      \n",
    "   \n",
    "    \n",
    "    fea_set = set()\n",
    "    for kk,vv in all_fea.iteritems():\n",
    "        if vv>minFreq:\n",
    "            fea_set.add(kk)\n",
    "    return df,fea_set            \n",
    "            \n",
    "def fea_col_proc(df_train_,df_test_,minFreq=10,q=0.9):    \n",
    "    train_df = df_train_.copy() # Maybe ther eis better way to avoid SettingWithCopyWarning \n",
    "    test_df = df_test_.copy()\n",
    "   \n",
    "    #---------------------------\n",
    "    #original length of the first feature, meant to capture those features typed in with wrong \n",
    "    # format -- all features are cramed into one phrase\n",
    "    \n",
    "    #-----------------------------\n",
    "    \n",
    "    def fea_clean(x): \n",
    "        if len(x) == 1:\n",
    "            tmp = x[0].strip('*').split('*')\n",
    "            if len(tmp) ==1:\n",
    "                tmp = tmp[0].split(u'\\u2022')\n",
    "            x = tmp\n",
    "        ret = [tt.encode('utf-8').decode('unicode_escape').encode('ascii','ignore').lower().strip() for tt in x]\n",
    "        return ret\n",
    "    train_df['features'] = train_df['features'].map(fea_clean)\n",
    "    test_df['features'] = test_df['features'].map(fea_clean)\n",
    "    \n",
    "    combined_fea = {'laundry in unit':set(['laundry in unit','in-unit washer/dryer','washer & dryer',\n",
    "                                    'washer/dryer','washer/dryer in unit']),\n",
    "                    'laundry in building':set(['laundry in building','laundry room',\n",
    "                                       'washer/dryer in building','on-site laundry']),\n",
    "                   'gym/fitness':set(['gym/fitness','fitness center','gym','gym in building']),\n",
    "                    'pre-war':set(['pre-war','prewar']),\n",
    "                    'live-in superintendent':set(['live-in superintendent','live-in super','live in super']),\n",
    "                    'hardwood floors':set(['hardwood floors','hardwood','hardwood floor','hardwood flooring']),\n",
    "                    'high ceiling':set(['high ceiling','high ceilings']),\n",
    "                    'full-time doorman':set(['full-time doorman','ft doorman','24/7 doorman','24 hour doorman',\n",
    "                                        '24-hour doorman','24hr doorman','full time doorman','24 hr doorman'])\n",
    "                   #'garage':['garage','parking']\n",
    "                }\n",
    "    \n",
    "            \n",
    "    train_df['low'] = train_df['interest_level'].map(lambda x:int(x==0))\n",
    "    train_df['medium'] = train_df['interest_level'].map(lambda x:int(x==1))\n",
    "    train_df['high'] = train_df['interest_level'].map(lambda x:int(x==2))\n",
    "    \n",
    "    train_df,tr_set = normalize_fea(train_df,combined_fea,minFreq)\n",
    "    test_df,te_set = normalize_fea(test_df,combined_fea,minFreq)\n",
    "    \n",
    "    word_set = tr_set.union(te_set)\n",
    "    \n",
    "    global_prob = np.array(train_df['interest_level'].value_counts()*1./len(train_df))\n",
    "    \n",
    "    allpairs = {'key':[],'kl distance':[],'low':[],'medium':[],'high':[],'count':[]}\n",
    "    for ww in word_set:\n",
    "        tmpdf = train_df[train_df['features'].map(lambda x:ww in x)]\n",
    "        p = np.array(tmpdf[['low','medium','high']].sum())*1.0/len(tmpdf)\n",
    "        kl_distance = entropy(p,global_prob)\n",
    "        allpairs['key'].append(ww)\n",
    "            #allpairs['similarity'].append(similarity)\n",
    "        allpairs['kl distance'].append(kl_distance)\n",
    "        allpairs['low'].append(p[0])\n",
    "        allpairs['medium'].append(p[1])\n",
    "        allpairs['high'].append(p[2])\n",
    "        allpairs['count'].append(len(tmpdf))\n",
    "    allpairs = pd.DataFrame(allpairs)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    allpairs = allpairs.sort_values('kl distance')\n",
    "    allpairs.to_csv('features_col.csv',index=False)\n",
    "    kl_quant = allpairs['kl distance'].quantile(q)\n",
    "    allpairs = allpairs[allpairs['kl distance']>kl_quant]\n",
    "    \n",
    "    fealist = list(allpairs['key'])\n",
    "    for ww in fealist:\n",
    "        train_df[ww] = train_df['features'].map(lambda x:int(ww in x))\n",
    "        test_df[ww] = test_df['features'].map(lambda x:int(ww in x))\n",
    "    return train_df,test_df,fealist,allpairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_features(train_df_,test_df_,col,minFreq=5,q=0.8):\n",
    "    train_df = train_df_.copy() \n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    \n",
    "    global_prob = np.array(train_df['interest_level'].value_counts()*1./len(train_df))\n",
    "    sizename = col + '_size' \n",
    "    gp = train_df.groupby(col).size()\n",
    "    gp.name = sizename\n",
    "    train_df = train_df.merge(gp.reset_index(),how='left')\n",
    "    \n",
    "    tmpdf = train_df[train_df[sizename]>minFreq].copy()\n",
    "\n",
    "    tmpdf['low'] = tmpdf['interest_level'].map(lambda x:int(x==0))\n",
    "    tmpdf['medium'] = tmpdf['interest_level'].map(lambda x:int(x==1))\n",
    "    tmpdf['high'] = tmpdf['interest_level'].map(lambda x:int(x==2))\n",
    "    tmpdf['total'] = 1.\n",
    "\n",
    "    tmpdf = tmpdf[[col,'low','medium','high','total']].groupby(col).sum()\n",
    "    tmpdf['low'] = tmpdf['low']/tmpdf['total']\n",
    "    tmpdf['high'] = tmpdf['high']/tmpdf['total']\n",
    "    tmpdf['medium'] = tmpdf['medium']/tmpdf['total']\n",
    "\n",
    "    allpairs = {'key':[],'kl distance':[],'low':[],'medium':[],'high':[],'count':[]}\n",
    "    for m_id,row in tmpdf.iterrows():\n",
    "        p = np.array(row[['low','medium','high']])\n",
    "        kl_distance = entropy(p,global_prob)\n",
    "        allpairs['key'].append(m_id)\n",
    "            #allpairs['similarity'].append(similarity)\n",
    "        allpairs['kl distance'].append(kl_distance)\n",
    "        allpairs['low'].append(p[0])\n",
    "        allpairs['medium'].append(p[1])\n",
    "        allpairs['high'].append(p[2])\n",
    "        allpairs['count'].append(row['total'])\n",
    "    \n",
    "    allpairs = pd.DataFrame(allpairs)\n",
    "    allpairs = allpairs.sort_values('kl distance')\n",
    "    allpairs.to_csv(col+'.csv',index=False)\n",
    "    \n",
    "    all_low = set(allpairs[allpairs['low']==1]['key'])\n",
    "    train_df['all_low'] = train_df[col].map(lambda x:int(x in all_low))\n",
    "    test_df['all_low'] = test_df[col].map(lambda x:int(x in all_low))\n",
    "    \n",
    "    allpairs = allpairs[allpairs['low']!=1]    \n",
    "    kl_quant = allpairs['kl distance'].quantile(q)\n",
    "    allpairs = allpairs[allpairs['kl distance']>kl_quant]\n",
    "    \n",
    "    fealist = ['all_low']\n",
    "    \n",
    "    alldf = train_df.append(test_df)\n",
    "    gp = alldf.groupby(col).size()\n",
    "    gp.name='size_info'\n",
    "    gp = gp.reset_index()\n",
    "    train_df = train_df.merge(gp,how='left')\n",
    "    test_df = test_df.merge(gp,how='left')\n",
    "    for ii in range(1,6):\n",
    "        train_df[col+'_size_'+str(ii)] = train_df['size_info'].map(lambda x:int(x==ii))\n",
    "        test_df[col+'_size_'+str(ii)] = test_df['size_info'].map(lambda x:int(x==ii))\n",
    "        fealist.append(col+'_size_'+str(ii))\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    ii = 0\n",
    "    for _,ww in allpairs['key'].iteritems():\n",
    "        train_df[col+ww] = train_df[col].map(lambda x:int(x==ww))\n",
    "        test_df[col+ww] = test_df[col].map(lambda x:int(x==ww))\n",
    "        fealist.append(col+ww)\n",
    "    train_df.index=train_df_.index\n",
    "    test_df.index=test_df_.index\n",
    "    return train_df,test_df,fealist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB_sklearn(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=10, num_rounds=5000,verbose=False):\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=num_rounds,\n",
    "                            objective='multi:softprob',\n",
    "                            learning_rate=0.02,\n",
    "                            max_depth=6,\n",
    "                            min_child_weight=1,\n",
    "                            subsample=.7,\n",
    "                            colsample_bytree=.7,\n",
    "                            colsample_bylevel=.5,\n",
    "                            gamma=0.005,\n",
    "                            scale_pos_weight=1,\n",
    "                            base_score=.5,\n",
    "                            #reg_lambda=0,\n",
    "                            #reg_alpha=0,\n",
    "                            #missing=0,\n",
    "                            seed=seed_val)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        clf.fit(train_X, train_y,eval_set=[(train_X, train_y), (test_X, test_y)],verbose=verbose,eval_metric='mlogloss',\n",
    "            early_stopping_rounds=50)\n",
    "    else:        \n",
    "        clf.fit(train_X, train_y,verbose=False)\n",
    "    pred_test_y = clf.predict_proba(test_X)\n",
    "    return pred_test_y, clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X=None, test_y=None, feature_names=None, seed_val=0, num_rounds=5000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['colsample_bylevel'] = 0.5\n",
    "    param['gamma'] = 0.005\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=30,verbose_eval=500)\n",
    "    pred_test_y = None\n",
    "    if test_X is not None:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        pred_test_y = model.predict(xgtest)\n",
    "    \n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def txt_feature(train_df,test_df,binary=True):\n",
    "    tr_id = train_df['listing_id'].copy()\n",
    "    te_id = test_df['listing_id'].copy() \n",
    "        \n",
    "    tr_df,te_df,fealist = id_features(train_df,test_df,'building_id',minFreq=5,q=0.8)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    if sum((tr_df.listing_id!=tr_id))>0 or sum((te_df.listing_id!=te_id))>0:\n",
    "        raise ValueError('listing ID cannot match!!!!!')\n",
    "    trainX4 = csr_matrix(tr_df[fealist].as_matrix())\n",
    "    testX4 = csr_matrix(te_df[fealist].as_matrix())\n",
    "    print 'building_id .... done'\n",
    "    \n",
    "    tr_df,te_df,fealist = id_features(train_df,test_df,'manager_id',minFreq=5,q=0.8)\n",
    "    \n",
    "    if sum((tr_df.listing_id!=tr_id))>0 or sum((te_df.listing_id!=te_id))>0:\n",
    "        raise ValueError('listing ID cannot match!!!!!')\n",
    "    trainX5 = csr_matrix(tr_df[fealist].as_matrix())\n",
    "    testX5 = csr_matrix(te_df[fealist].as_matrix())\n",
    "    print 'manager_id .... done'\n",
    "    \n",
    "    tr_df,te_df,fealist,allpairs = fea_col_proc(train_df,test_df,minFreq=10,q=0.3)\n",
    "    if sum((tr_df.listing_id!=tr_id))>0 or sum((te_df.listing_id!=te_id))>0:\n",
    "        raise ValueError('listing ID cannot match!!!!!')\n",
    "    trainX3 = csr_matrix(tr_df[fealist].as_matrix())\n",
    "    testX3 = csr_matrix(te_df[fealist].as_matrix())\n",
    "    print 'fea column... done'\n",
    "    \n",
    "    addr_fea = AddressFeature()\n",
    "    train_df,test_df,addr_fealist = addr_fea.transform(train_df,test_df)\n",
    "    global_prob = np.array(train_df['interest_level'].value_counts()*1./len(train_df))\n",
    "    trainX2,testX2,key2 = indicative_keyword(train_df,test_df,'street_address',global_prob,10,0.95,verbose=True)\n",
    "    print 'street_address... done'\n",
    "    trainX1,testX1,key1 = indicative_keyword(train_df,test_df,'description',global_prob,20,0.95,verbose=False)    \n",
    "    print 'description .... done'\n",
    "    \n",
    "    \n",
    "    if binary:\n",
    "        try:\n",
    "            trainX = csr_matrix(hstack([trainX1,trainX2,trainX3,trainX4,trainX5])>0,dtype='int')\n",
    "            testX = csr_matrix(hstack([testX1,testX2,testX3,testX4,testX5])>0,dtype='int')\n",
    "        except:\n",
    "            return [trainX1,trainX2,trainX3,trainX4,trainX5],[testX1,testX2,testX3,testX4,testX5]\n",
    "    \n",
    "    \n",
    "    return trainX,testX\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df,test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id .... done\n",
      "manager_id .... done\n",
      "fea column... done\n",
      "cannot find 501 west\n",
      "cannot find 47 east\n",
      "cannot find 80th moore\n",
      "cannot find 250th 10th\n",
      "street_address... done\n",
      "description .... done\n"
     ]
    }
   ],
   "source": [
    "trainX,testX = txt_feature(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>7211212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5465</td>\n",
       "      <td>7150865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2850</td>\n",
       "      <td>6887163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3275</td>\n",
       "      <td>6888711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3350</td>\n",
       "      <td>6934781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bathrooms  bedrooms  price  listing_id\n",
       "10            1.5         3   3000     7211212\n",
       "10000         1.0         2   5465     7150865\n",
       "100004        1.0         1   2850     6887163\n",
       "100007        1.0         1   3275     6888711\n",
       "100013        1.0         4   3350     6934781"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[corefea].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bf = BasicFeature()\n",
    "tr_df,te_df,basicfealist = bf.transform(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corefea = ['bathrooms','bedrooms','price','listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional = tr_df[basicfealist+['bathrooms','bedrooms','price','listing_id']].fillna(0).as_matrix()\n",
    "additional = preprocessing.normalize(additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainX,testX) = cPickle.load(open('all_cate.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainXex = np.hstack([trainX.todense(),additional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp=csr_matrix(trainXex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainXex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 1736)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainXex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_cv(train_X,train_y):\n",
    "    \n",
    "    cv_scores = [] \n",
    "    #print fea_categorical\n",
    "    #print fea_additional\n",
    "    #print feature_params\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=22)\n",
    "    for dev_index, val_index in kf.split(train_X,train_y):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "        #print('best iterations:{}, best_score={}, last_score={}'.format(model.best_iteration,\n",
    "        #                                                           model.best_score,log_loss(val_y, preds)))\n",
    "        #importance_inx = np.argsort(model.feature_importances_*-1)\n",
    "        #print('Most important 40 features:')\n",
    "        #ff = [(fealist[x],model.feature_importances_[x]) for x in importance_inx[:40]]\n",
    "        #print(ff)\n",
    "        #print('-------------------------')\n",
    "            \n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print cv_scores\n",
    "    print 'mean score={}'.format(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08586\ttest-mlogloss:1.08614\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[500]\ttrain-mlogloss:0.535448\ttest-mlogloss:0.583\n",
      "[1000]\ttrain-mlogloss:0.497282\ttest-mlogloss:0.571165\n",
      "[1500]\ttrain-mlogloss:0.46744\ttest-mlogloss:0.565979\n",
      "[2000]\ttrain-mlogloss:0.440996\ttest-mlogloss:0.563331\n",
      "Stopping. Best iteration:\n",
      "[2186]\ttrain-mlogloss:0.431822\ttest-mlogloss:0.562622\n",
      "\n",
      "[0.56265075859289171]\n",
      "[0]\ttrain-mlogloss:1.0859\ttest-mlogloss:1.086\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[500]\ttrain-mlogloss:0.536159\ttest-mlogloss:0.581824\n",
      "[1000]\ttrain-mlogloss:0.497542\ttest-mlogloss:0.568898\n",
      "[1500]\ttrain-mlogloss:0.468479\ttest-mlogloss:0.563199\n",
      "[2000]\ttrain-mlogloss:0.441951\ttest-mlogloss:0.560062\n",
      "[2500]\ttrain-mlogloss:0.416508\ttest-mlogloss:0.557827\n",
      "Stopping. Best iteration:\n",
      "[2866]\ttrain-mlogloss:0.397855\ttest-mlogloss:0.557139\n",
      "\n",
      "[0.56265075859289171, 0.5571979496552516]\n",
      "[0]\ttrain-mlogloss:1.08591\ttest-mlogloss:1.08609\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[500]\ttrain-mlogloss:0.536437\ttest-mlogloss:0.580088\n",
      "[1000]\ttrain-mlogloss:0.498059\ttest-mlogloss:0.566572\n",
      "[1500]\ttrain-mlogloss:0.468366\ttest-mlogloss:0.560572\n",
      "[2000]\ttrain-mlogloss:0.44188\ttest-mlogloss:0.55729\n",
      "[2500]\ttrain-mlogloss:0.416332\ttest-mlogloss:0.555397\n",
      "Stopping. Best iteration:\n",
      "[2659]\ttrain-mlogloss:0.408801\ttest-mlogloss:0.554941\n",
      "\n",
      "[0.56265075859289171, 0.5571979496552516, 0.55496164475288945]\n",
      "[0]\ttrain-mlogloss:1.08588\ttest-mlogloss:1.08608\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[500]\ttrain-mlogloss:0.536118\ttest-mlogloss:0.582563\n",
      "[1000]\ttrain-mlogloss:0.497579\ttest-mlogloss:0.569854\n",
      "[1500]\ttrain-mlogloss:0.467502\ttest-mlogloss:0.563971\n",
      "[2000]\ttrain-mlogloss:0.441833\ttest-mlogloss:0.561081\n",
      "Stopping. Best iteration:\n",
      "[2191]\ttrain-mlogloss:0.432618\ttest-mlogloss:0.560211\n",
      "\n",
      "[0.56265075859289171, 0.5571979496552516, 0.55496164475288945, 0.56024058805805177]\n",
      "[0]\ttrain-mlogloss:1.08588\ttest-mlogloss:1.08604\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 30 rounds.\n",
      "[500]\ttrain-mlogloss:0.535244\ttest-mlogloss:0.582214\n",
      "[1000]\ttrain-mlogloss:0.495541\ttest-mlogloss:0.570112\n",
      "[1500]\ttrain-mlogloss:0.46613\ttest-mlogloss:0.565059\n",
      "[2000]\ttrain-mlogloss:0.439458\ttest-mlogloss:0.562516\n",
      "Stopping. Best iteration:\n",
      "[2073]\ttrain-mlogloss:0.435993\ttest-mlogloss:0.562263\n",
      "\n",
      "[0.56265075859289171, 0.5571979496552516, 0.55496164475288945, 0.56024058805805177, 0.56228870334932179]\n",
      "mean score=0.559467928882\n"
     ]
    }
   ],
   "source": [
    "simple_cv(trainXex,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_cv(trainX,train_y):\n",
    "    cv_scores = []\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=22)\n",
    "    for dev_index, val_index in kf.split(trainX,train_y):\n",
    "        dev_X, val_X = trainX[dev_index,:], trainX[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        clf = LogisticRegressionCV()\n",
    "        clf.fit(dev_X,dev_y)\n",
    "        preds = clf.predict_proba(val_X)\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print cv_scores\n",
    "    print 'mean score={}'.format(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(train_df['interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66002929165713287]\n",
      "[0.66002929165713287, 0.65937962640304315]\n",
      "[0.66002929165713287, 0.65937962640304315, 0.65359584828266015]\n",
      "[0.66002929165713287, 0.65937962640304315, 0.65359584828266015, 0.65828547793275993]\n",
      "[0.66002929165713287, 0.65937962640304315, 0.65359584828266015, 0.65828547793275993, 0.65003758936338873]\n",
      "mean score=0.656265566728\n"
     ]
    }
   ],
   "source": [
    "simple_cv(tmp,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cPickle.dump((trainX,testX),open('all_cate.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 1717)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 1717)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean score=0.647136936245\n"
     ]
    }
   ],
   "source": [
    "simple_cv(trainX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1647"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['all_low'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = train_df[fealist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10962"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find 501 west\n",
      "cannot find 47 east\n",
      "cannot find 80th moore\n",
      "cannot find 250th 10th\n",
      "> <ipython-input-109-4f75f7c3f757>(50)indicative_keyword()\n",
      "-> testX = count_vect.transform(test_df[col])\n",
      "(Pdb) trainX.shape\n",
      "(49352, 130)\n",
      "(Pdb) allpairs.head(20)\n",
      "     count      high             key  kl distance       low    medium\n",
      "7       16  0.000000    12nd charles     0.364300  1.000000  0.000000\n",
      "37      12  0.000000      361st east     0.364300  1.000000  0.000000\n",
      "41      22  0.227273           533rd     0.309781  0.318182  0.454545\n",
      "64      20  0.400000           597th     0.550221  0.250000  0.350000\n",
      "80      22  0.000000      1160th 5th     0.364300  1.000000  0.000000\n",
      "84      23  0.173913    moore street     0.398730  0.260870  0.565217\n",
      "85      11  0.000000      10 seagirt     0.590113  0.272727  0.727273\n",
      "93      18  0.277778  hudson parkway     0.396521  0.277778  0.444444\n",
      "124     15  0.000000       103rd 5th     0.364300  1.000000  0.000000\n",
      "142     13  0.230769     king street     0.326815  0.307692  0.461538\n",
      "162     25  0.000000      162nd west     0.364300  1.000000  0.000000\n",
      "190     11  0.272727      511st east     0.401691  0.272727  0.454545\n",
      "208     11  0.363636        213 east     0.631625  0.181818  0.454545\n",
      "214     14  0.214286       533rd 3rd     0.356959  0.285714  0.500000\n",
      "224     11  0.363636    218th street     0.476293  0.272727  0.363636\n",
      "234     13  0.307692           920th     0.374451  0.307692  0.384615\n",
      "282     15  0.400000           299th     0.408402  0.400000  0.200000\n",
      "290     14  0.000000   241st central     0.364300  1.000000  0.000000\n",
      "299     13  0.000000      303rd west     0.364300  1.000000  0.000000\n",
      "306     11  0.272727      352nd east     0.401691  0.272727  0.454545\n",
      "(Pdb) n\n",
      "> <ipython-input-109-4f75f7c3f757>(51)indicative_keyword()\n",
      "-> return trainX,testX,allpairs\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-109-4f75f7c3f757>(51)indicative_keyword()->(<49352x1...w format>, <74659x1...w format>,       co... columns])\n",
      "-> return trainX,testX,allpairs\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-110-10859c3963b1>(7)txt_feature()->None\n",
      "-> trainX2,testX2,key2 = indicative_keyword(train_df,test_df,'street_address',global_prob,10,0.95,verbose=True)\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-599f10c1709b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mohoh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-10859c3963b1>\u001b[0m in \u001b[0;36mtxt_feature\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#trainX1,testX1,key1 = indicative_keyword(train_df,test_df,'description',global_prob,20,0.95,verbose=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrainX2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindicative_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'street_address'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'return'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_return\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_returning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ohoh = txt_feature(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = csr_matrix(aa>0,dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 1858)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allpairs = hehe[hehe['kl distance']>hehe['kl distance'].quantile(0.95)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2),stop_words=None,min_df = 20,vocabulary=list(allpairs['key']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['description'] = train_df['description'].map(lambda x:re.sub('\\W',' ',x))\n",
    "train_df['description'] = train_df['description'].map(lambda x:' '.join(x.split()))\n",
    "test_df['description'] = test_df['description'].map(lambda x:re.sub('\\W',' ',x))\n",
    "test_df['description'] = test_df['description'].map(lambda x:' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = count_vect.transform(train_df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addr_fea = AddressFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = aa.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[matrix([[2, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heapq.nlargest(10,tmp.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nima = tmp.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tma = nima[nima==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32177)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hehe.to_csv('description_fea.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f925f37b7d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92590033d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hehe['kl distance'].hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0261667686562274"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hehe['kl distance'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2906845791282257"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hehe['kl distance'].quantile(0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464.40000000000003"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hehe)*.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
