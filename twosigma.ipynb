{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_path='../input/'):\n",
    "    \n",
    "    train_file = data_path + \"train.json\"\n",
    "    test_file = data_path + \"test.json\"\n",
    "    train_df = pd.read_json(train_file)\n",
    "    test_df = pd.read_json(test_file)\n",
    "    interest_map = {'low':0,'medium':1,'high':2}\n",
    "    train_df['interest_level'] = train_df['interest_level'].map(interest_map)\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    train_df[\"street_address\"] = train_df['street_address'].apply(fmt)\n",
    "    train_df[\"display_address\"] = train_df[\"display_address\"].apply(fmt)\n",
    "    \n",
    "    test_df[\"street_address\"] = test_df['street_address'].apply(fmt)\n",
    "    test_df[\"display_address\"] = test_df[\"display_address\"].apply(fmt)\n",
    "            \n",
    "    return train_df,test_df\n",
    "\n",
    "def write_output(preds,test_df,prefix=''):\n",
    "    out_df = pd.DataFrame(preds)\n",
    "    out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "    out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "    \n",
    "    import time\n",
    "    filename = prefix + time.strftime(\"%m.%d.\") + '.csv'\n",
    "    out_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_feature(train_df_,test_df_):    \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    train_df[\"created\"], test_df[\"created\"] = pd.to_datetime(train_df[\"created\"]), pd.to_datetime(test_df[\"created\"])\n",
    "    test_df['days'], train_df['days'] = test_df['created']-pd.to_datetime('2016-04-01'), train_df['created'] - pd.to_datetime('2016-04-01')\n",
    "    test_df['hours'], train_df['hours'] = test_df['days']/np.timedelta64(1,'h'), train_df['days']/np.timedelta64(1,'h')\n",
    "    test_df['days'], train_df['days'] = test_df['days']/np.timedelta64(1, 'D'), train_df['days']/np.timedelta64(1,'D')\n",
    "    \n",
    "    train_df['hours'], test_df['hours'] = train_df['hours'].map(int), test_df['hours'].map(int)\n",
    "    gp = train_df.append(test_df).groupby('hours').size()\n",
    "    gp.name = 'hour_size'\n",
    "    gp = gp.reset_index()\n",
    "    train_df = pd.merge(train_df,gp,on='hours',how='left')\n",
    "    test_df = pd.merge(test_df,gp,on='hours',how='left')\n",
    "    del test_df['hours']; del train_df['hours']\n",
    "    \n",
    "    train_df['weekdays'], test_df['weekdays'] = train_df['created'].map(lambda x:x.weekday()), test_df['created'].map(lambda x:x.weekday())\n",
    "\n",
    "    # Features from date columns #\n",
    "    train_df[\"created_month\"], test_df[\"created_month\"] = train_df[\"created\"].dt.month, test_df[\"created\"].dt.month\n",
    "    train_df[\"created_day\"], test_df[\"created_day\"] = train_df[\"created\"].dt.day, test_df[\"created\"].dt.day\n",
    "    train_df[\"created_hour\"], test_df[\"created_hour\"]= train_df[\"created\"].dt.hour, test_df[\"created\"].dt.hour\n",
    "    # count of photos #\n",
    "    train_df[\"num_photos\"], test_df[\"num_photos\"]  = train_df[\"photos\"].apply(len), test_df[\"photos\"].apply(len)\n",
    "\n",
    "    # count of \"features\" #\n",
    "    train_df[\"num_features\"], test_df[\"num_features\"] = train_df[\"features\"].apply(len), test_df[\"features\"].apply(len)\n",
    "\n",
    "    # count of words present in description column #\n",
    "    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    \n",
    "    tmp = train_df.append(test_df)\n",
    "    gp = tmp.groupby('manager_id').size()\n",
    "    gp.name = 'manager_count'\n",
    "    gp = gp.reset_index()\n",
    "    train_df, test_df = pd.merge(train_df,gp,how='left'), pd.merge(test_df,gp,how='left')\n",
    "    \n",
    "    gp = tmp.groupby('building_id').size()\n",
    "    gp.name = 'building_count'\n",
    "    gp = gp.reset_index()\n",
    "    train_df, test_df = pd.merge(train_df,gp,how='left'), pd.merge(test_df,gp,how='left')\n",
    "    \n",
    "    train_df['zero_building_id'] = train_df['building_id'].map(lambda x:int(x=='0'))\n",
    "    test_df['zero_building_id'] = test_df['building_id'].map(lambda x:int(x=='0'))\n",
    "    fea_list = ['zero_building_id','weekdays',\"num_features\",\"num_description_words\",\"days\",\"num_photos\", \"created_month\", \"created_day\", \"created_hour\"]\n",
    "    \n",
    "    return train_df,test_df,fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residue_fea(train_df,test_df):\n",
    "    if 'days' not in train_df.columns:\n",
    "        train_df[\"created\"], test_df[\"created\"] = pd.to_datetime(train_df[\"created\"]), pd.to_datetime(test_df[\"created\"])\n",
    "        test_df['days'], train_df['days'] = test_df['created']-pd.to_datetime('2016-04-01'), train_df['created'] - pd.to_datetime('2016-04-01')\n",
    "        \n",
    "    train_df['dayx20'] = train_df['days'].map(lambda x:int(x*20))\n",
    "    gp = train_df.groupby('dayx20',as_index=False)['listing_id'].min()\n",
    "    gp['dayx20'] = gp['dayx20']/20.\n",
    "    \n",
    "    lin = LinearRegression()\n",
    "    X = gp['dayx20'].as_matrix().reshape(len(gp),1)\n",
    "    y = np.array(gp['listing_id'])\n",
    "    lin.fit(X,y)\n",
    "    \n",
    "    train_df['listing_id_residue'] = train_df['days'].map(lambda x:x*lin.coef_[0] + lin.intercept_)\n",
    "    train_df['listing_id_residue'] = train_df['listing_id'] - train_df['listing_id_residue'] \n",
    "    test_df['listing_id_residue'] = test_df['days'].map(lambda x:x*lin.coef_[0] + lin.intercept_)\n",
    "    test_df['listing_id_residue'] = test_df['listing_id'] - train_df['listing_id_residue'] \n",
    "    \n",
    "    train_df['int_days'] = train_df['days'].map(lambda x:int(x))\n",
    "    train_df['intra'] = train_df['days'] - train_df['int_days']\n",
    "    train_df['intra'] = train_df['intra'].map(lambda x:int(x>0.3))\n",
    "    \n",
    "    test_df['int_days'] = test_df['days'].map(lambda x:int(x))\n",
    "    test_df['intra'] = test_df['days'] - train_df['int_days']\n",
    "    test_df['intra'] = test_df['intra'].map(lambda x:int(x>0.3))\n",
    "    del train_df['dayx20']\n",
    "    return train_df,test_df,['int_days','intra','listing_id_residue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def careful_avg(train_df_,test_df_,agg_keys,mask,suffix,debug=False):\n",
    "    \"\"\"\n",
    "    agg_keys: the list of column names groupby is based on, e.g.['manager_id','bedrooms','bathrooms']\n",
    "    mask has same length of train_df, used for cross validation\n",
    "    mask=1 training, masking=0 validation\n",
    "    suffix is for the resulted feature names\n",
    "    \"\"\"\n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    train_df['mask'] = mask\n",
    "    train_df['target'] = train_df['interest_level']\n",
    "    train_df.loc[train_df['mask']==0,'target'] = -1\n",
    "    alldf = train_df.append(test_df)\n",
    "    alldf['target'] = alldf['target'].fillna(-1)\n",
    "    alldf['mask'] = alldf['mask'].fillna(0)\n",
    "    \n",
    "    if debug:\n",
    "        debug_list =  set(['401 w 25th st.','30 west 63rd street'])\n",
    "        alldf = alldf[alldf['street_address'].isin(debug_list)]\n",
    "        import pdb;pdb.set_trace()\n",
    "    \n",
    "    global_avg = train_df[train_df['mask']==1].groupby('target').count()*1.0/np.sum(mask)\n",
    "    columns = ['#records_'+suffix,'#train_record_'+suffix,'mean_price_apt_'+suffix,'low_avg_'+suffix,\n",
    "               'med_avg_'+suffix,'high_avg_'+suffix]\n",
    "    low_avg_name = 'low_avg_'+suffix\n",
    "    med_avg_name = 'med_avg_'+suffix\n",
    "    high_avg_name = 'high_avg_'+suffix\n",
    "    gp = alldf.groupby(agg_keys)\n",
    "    res = gp['mask'].agg({'#train_record_'+suffix:np.sum,\n",
    "                         '#records_'+suffix:np.size})\n",
    "    res['mean_price_apt_'+suffix] = gp['price'].mean()\n",
    "    res2 = gp['target'].agg({low_avg_name:lambda x:np.sum(x==0)*1.,\n",
    "                            med_avg_name:lambda x:np.sum(x==1)*1.,\n",
    "                            high_avg_name:lambda x:np.sum(x==2)*1.,\n",
    "                            'total':lambda x:np.sum(x!=-1)})\n",
    "    res = res.join(res2)\n",
    "    res = res.reset_index()\n",
    "    res = pd.merge(alldf[['listing_id','mask','target'] + agg_keys],res,on=agg_keys,how='left')\n",
    "    res.loc[res['target']==0,low_avg_name] = res.loc[res['target']==0,low_avg_name] - 1\n",
    "    res.loc[res['target']==1,med_avg_name] = res.loc[res['target']==1,med_avg_name] - 1\n",
    "    res.loc[res['target']==2,high_avg_name] = res.loc[res['target']==2,high_avg_name] - 1\n",
    "    res.loc[res['target']>=0,'total'] = res.loc[res.target>=0,'total'] - 1\n",
    "    res.loc[res['total']>0,low_avg_name] = res.loc[res['total']>0,low_avg_name]/res.loc[res['total']>0,'total']\n",
    "    res.loc[res['total']>0,med_avg_name] = res.loc[res['total']>0,med_avg_name]/res.loc[res['total']>0,'total']\n",
    "    res.loc[res['total']>0,high_avg_name] = res.loc[res['total']>0,high_avg_name]/res.loc[res['total']>0,'total']\n",
    "    res.loc[res['total']==0,low_avg_name] = res.loc[res['total']>0,low_avg_name].mean()\n",
    "    res.loc[res['total']==0,med_avg_name] = res.loc[res['total']>0,med_avg_name].mean()\n",
    "    res.loc[res['total']==0,high_avg_name] = res.loc[res['total']>0,high_avg_name].mean()\n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "        res.to_csv('debug_avg.csv',index=False,encoding='utf8')\n",
    "        raise NameError('Debug exit')\n",
    "    train_df = pd.merge(train_df,res[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    test_df = pd.merge(test_df,res[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    del train_df['mask']\n",
    "    del train_df['target']\n",
    "    \n",
    "    return train_df,test_df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def past_future(train_df_,test_df_,agg_keys,mask,suffix,debug=False):\n",
    "    \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    train_df['mask'] = mask\n",
    "    train_df['target'] = train_df['interest_level']\n",
    "    train_df.loc[train_df['mask']==0,'target'] = -1\n",
    "    alldf = train_df.append(test_df)\n",
    "    alldf['target'] = alldf['target'].fillna(-1)\n",
    "    alldf['mask'] = alldf['mask'].fillna(0)\n",
    "    if debug:\n",
    "        debug_list =  set(['401 w 25th st.','30 west 63rd street'])\n",
    "        alldf = alldf[alldf['street_address'].isin(debug_list)]\n",
    "        import pdb;pdb.set_trace()\n",
    "    if 'days' not in alldf.columns:\n",
    "        raise NameError(\"This function should be called after basic_feature()\")\n",
    "    alldf = alldf.sort_values('days')\n",
    "    alldf.index=range(len(alldf))\n",
    "    #gp = alldf.groupby(['manager_id','building_id','bathrooms','bedrooms','street_address'])\n",
    "    gp = alldf.groupby(agg_keys)\n",
    "    alldf['last_listing_day_'+suffix] = gp['days'].transform(lambda x:x.diff(1)).fillna(0)    \n",
    "    alldf['next_listing_days_'+suffix] = gp['days'].transform(lambda x:x.diff(-1)).fillna(0)    \n",
    "    alldf['last_listing_price_'+suffix] = gp['price'].transform(lambda x:x.diff(1)).fillna(0)\n",
    "    alldf['next_listing_price_'+suffix] = gp['price'].transform(lambda x:x.diff(-1)).fillna(0)\n",
    "    \n",
    "    alldf['last_listing_interest_'+suffix] = gp['target'].transform(lambda x:x.shift(1)).fillna(-1)\n",
    "    alldf['next_listing_interest_'+suffix] = gp['target'].transform(lambda x:x.shift(-1)).fillna(-1)\n",
    "    \n",
    "    if debug:\n",
    "        alldf.to_csv('debug.csv',index=False,encoding='utf8')\n",
    "        raise NameError('Exit by Debug')\n",
    "        \n",
    "    columns = ['last_listing_day_'+suffix,'next_listing_days_'+suffix,'last_listing_price_'+suffix,\n",
    "              'next_listing_price_'+suffix,'last_listing_interest_'+suffix,'next_listing_interest_'+suffix]\n",
    "    \n",
    "    train_df = pd.merge(train_df,alldf[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    test_df = pd.merge(test_df,alldf[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    del train_df['mask']\n",
    "    del train_df['target']\n",
    "    return train_df,test_df,columns\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB_sklearn(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=10, num_rounds=5000):\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=num_rounds,\n",
    "                            objective='multi:softprob',\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=6,\n",
    "                            min_child_weight=1,\n",
    "                            subsample=.7,\n",
    "                            colsample_bytree=.7,\n",
    "                            colsample_bylevel=.5,\n",
    "                            gamma=0.005,\n",
    "                            scale_pos_weight=1,\n",
    "                            base_score=.5,\n",
    "                            #reg_lambda=0,\n",
    "                            #reg_alpha=0,\n",
    "                            #missing=0,\n",
    "                            seed=seed_val)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        clf.fit(train_X, train_y,eval_set=[(train_X, train_y), (test_X, test_y)],verbose=False,eval_metric='mlogloss',\n",
    "            early_stopping_rounds=50)\n",
    "    else:        \n",
    "        clf.fit(train_X, train_y,verbose=False)\n",
    "    pred_test_y = clf.predict_proba(test_X)\n",
    "    return pred_test_y, clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    created  listing_id  price\n",
      "10      2016-06-24 07:54:24     7211212   3000\n",
      "10000   2016-06-12 12:19:27     7150865   5465\n",
      "100004  2016-04-17 03:26:41     6887163   2850\n",
      "100007  2016-04-18 02:22:02     6888711   3275\n",
      "100013  2016-04-28 01:32:41     6934781   3350\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df = read_data()\n",
    "print train_df[print_col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df,basic_fealist = basic_feature(train_df,test_df)\n",
    "train_df,test_df,residue_fealist = residue_fea(train_df,test_df)\n",
    "print train_df[print_col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "basic ---\n",
      "(39481, 18)\n",
      "0.568262642948\n",
      "more -----\n",
      "(39481, 24)\n",
      "0.564130794619\n",
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "basic ---\n",
      "(39481, 18)\n",
      "0.575044504553\n",
      "more -----\n",
      "(39481, 24)\n",
      "0.570477343157\n",
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "basic ---\n",
      "(39481, 18)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-d0587117c2d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#cv_scores.append(log_loss(y_test,preds)); print(cv_scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'basic ---'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mrunone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfealist0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'more -----'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-d0587117c2d6>\u001b[0m in \u001b[0;36mrunone\u001b[0;34m(train_df, train_y, mask, fealist)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfealist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunXGB_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-9b3441fe02b2>\u001b[0m in \u001b[0;36mrunXGB_sklearn\u001b[0;34m(train_X, train_y, test_X, test_y, feature_names, seed_val, num_rounds)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         clf.fit(train_X, train_y,eval_set=[(train_X, train_y), (test_X, test_y)],verbose=False,eval_metric='mlogloss',\n\u001b[0;32m---> 21\u001b[0;31m             early_stopping_rounds=50)\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=22)\n",
    "train_df.index=range(len(train_df))\n",
    "train_y = train_df['interest_level'].as_matrix().ravel()\n",
    "ii = 1\n",
    "cv_scores=[]\n",
    "core_feature = ['bathrooms','bedrooms','latitude','longitude','price','listing_id']\n",
    "print_col = ['created','listing_id','price']\n",
    "for dev_index,val_index in kf.split(train_df[['listing_id','price']].as_matrix(),train_y):\n",
    "    train_df_ = train_df.copy()\n",
    "    test_df_ = test_df.copy()\n",
    "    \n",
    "    mask = np.ones(len(train_df))\n",
    "    mask[val_index] = 0\n",
    "    #train_df,test_df,fea_avg_building_bed_bath = careful_avg(train_df,test_df,\n",
    "    #                                agg_keys=['building_id','street_address','bedrooms','bathrooms'],\n",
    "    #                                mask=mask,suffix='building_bed_bath')\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    print train_df_[print_col].head(5)\n",
    "    #train_df,test_df,fea_avg_building = careful_avg(train_df,test_df,agg_keys=['building_id'],\n",
    "    #                                   mask=mask,suffix='building')\n",
    "    #print train_df[print_col].head(5)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #train_df,test_df,fea_avg_manager = careful_avg(train_df,test_df,agg_keys=['manager_id'],\n",
    "    #                                   mask=mask,suffix='manager')\n",
    "    #print train_df[print_col].head(5)\n",
    "    tr_df,te_df,fea_past_fut_manager_building = past_future(train_df_,test_df_,\n",
    "                                    agg_keys=['manager_id','building_id','bathrooms','bedrooms','street_address'],\n",
    "                                    mask=mask,suffix='manager_building')\n",
    "    #tr_df,te_df,fea_past_fut_building = past_future(train_df,test_df,\n",
    "    #                                agg_keys=['building_id','bathrooms','bedrooms'],\n",
    "    #                                mask=mask,suffix='building')\n",
    "    \n",
    "    #tr_df,te_df,past_fut_manager_fealist = past_future(train_df,test_df,mask=mask,debug=True)\n",
    "    \n",
    "    print tr_df[print_col].head(5)\n",
    "\n",
    "    fealist = core_feature + basic_fealist + residue_fealist + fea_avg_building_bed_bath + fea_avg_building + fea_avg_manager + fea_past_fut_manager_building + fea_past_fut_building\n",
    "    fealist = core_feature + basic_fealist + residue_fealist  + fea_avg_building + fea_avg_manager + fea_past_fut_manager_building \n",
    "    fealist0 = core_feature + basic_fealist + residue_fealist #+ fea_past_fut_manager_building\n",
    "    fealist1 = core_feature + basic_fealist + residue_fealist + fea_past_fut_manager_building\n",
    "    def runone(train_df,train_y,mask,fealist):\n",
    "        X_train = train_df.loc[mask==1,fealist]\n",
    "        print X_train.shape\n",
    "        y_train = train_y[mask==1]\n",
    "        X_test = train_df.loc[mask==0,fealist]\n",
    "        y_test = train_y[mask==0]\n",
    "        preds,model = runXGB_sklearn(X_train,y_train,X_test,y_test)\n",
    "        print log_loss(y_test,preds)\n",
    "    \n",
    "    #print('best iterations:{}, best_score={}, last_score={}'.format(model.best_iteration,\n",
    "#                                                                   model.best_score,log_loss(y_test, preds)))\n",
    "    #importance_inx = np.argsort(model.feature_importances_*-1)\n",
    "    #print('Most important 40 features:')\n",
    "    #ff = [(fealist[x],model.feature_importances_[x]) for x in importance_inx[:40]]\n",
    "    #print(ff)\n",
    "    \n",
    "    #cv_scores.append(log_loss(y_test,preds)); print(cv_scores)\n",
    "    print 'basic ---'\n",
    "    runone(tr_df,train_y,mask,fealist0)\n",
    "    \n",
    "    print 'more -----'\n",
    "    runone(tr_df,train_y,mask,fealist1)\n",
    "    \n",
    "    \n",
    "print 'mean score={}'.format(np.mean(cv_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 18)\n",
    "0.568262642948\n",
    "more -----\n",
    "(39481, 22)\n",
    "0.566260937986\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 18)\n",
    "0.575044504553\n",
    "more -----\n",
    "(39481, 22)\n",
    "0.573613910933\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 18)\n",
    "0.564224674179\n",
    "more -----\n",
    "(39481, 22)\n",
    "0.562056812539\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 18)\n",
    "0.572112259968\n",
    "more -----\n",
    "(39481, 22)\n",
    "0.570431576498\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39484, 18)\n",
    "0.574043627235\n",
    "more -----\n",
    "(39484, 22)\n",
    "0.572923416641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
