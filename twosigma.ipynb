{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from feature_extraction import *\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_path='../input/'):\n",
    "    \n",
    "    train_file = data_path + \"train.json\"\n",
    "    test_file = data_path + \"test.json\"\n",
    "    train_df = pd.read_json(train_file)\n",
    "    test_df = pd.read_json(test_file)\n",
    "    interest_map = {'low':0,'medium':1,'high':2}\n",
    "    train_df['interest_level'] = train_df['interest_level'].map(interest_map)\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    train_df[\"street_address\"] = train_df['street_address'].apply(fmt)\n",
    "    train_df[\"display_address\"] = train_df[\"display_address\"].apply(fmt)\n",
    "    \n",
    "    test_df[\"street_address\"] = test_df['street_address'].apply(fmt)\n",
    "    test_df[\"display_address\"] = test_df[\"display_address\"].apply(fmt)\n",
    "            \n",
    "    return train_df,test_df\n",
    "\n",
    "def write_output(preds,test_df,prefix=''):\n",
    "    out_df = pd.DataFrame(preds)\n",
    "    out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "    out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "    \n",
    "    import time\n",
    "    filename = prefix + time.strftime(\"%m.%d.\") + '.csv'\n",
    "    out_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_feature(train_df_,test_df_):    \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    train_df[\"created\"], test_df[\"created\"] = pd.to_datetime(train_df[\"created\"]), pd.to_datetime(test_df[\"created\"])\n",
    "    test_df['days'], train_df['days'] = test_df['created']-pd.to_datetime('2016-04-01'), train_df['created'] - pd.to_datetime('2016-04-01')\n",
    "    test_df['hours'], train_df['hours'] = test_df['days']/np.timedelta64(1,'h'), train_df['days']/np.timedelta64(1,'h')\n",
    "    test_df['days'], train_df['days'] = test_df['days']/np.timedelta64(1, 'D'), train_df['days']/np.timedelta64(1,'D')\n",
    "    \n",
    "    train_df['hours'], test_df['hours'] = train_df['hours'].map(int), test_df['hours'].map(int)\n",
    "    gp = train_df.append(test_df).groupby('hours').size()\n",
    "    gp.name = 'hour_size'\n",
    "    gp = gp.reset_index()\n",
    "    train_df = pd.merge(train_df,gp,on='hours',how='left')\n",
    "    test_df = pd.merge(test_df,gp,on='hours',how='left')\n",
    "    del test_df['hours']; del train_df['hours']\n",
    "    \n",
    "    train_df['weekdays'], test_df['weekdays'] = train_df['created'].map(lambda x:x.weekday()), test_df['created'].map(lambda x:x.weekday())\n",
    "\n",
    "    # Features from date columns #\n",
    "    train_df[\"created_month\"], test_df[\"created_month\"] = train_df[\"created\"].dt.month, test_df[\"created\"].dt.month\n",
    "    train_df[\"created_day\"], test_df[\"created_day\"] = train_df[\"created\"].dt.day, test_df[\"created\"].dt.day\n",
    "    train_df[\"created_hour\"], test_df[\"created_hour\"]= train_df[\"created\"].dt.hour, test_df[\"created\"].dt.hour\n",
    "    # count of photos #\n",
    "    train_df[\"num_photos\"], test_df[\"num_photos\"]  = train_df[\"photos\"].apply(len), test_df[\"photos\"].apply(len)\n",
    "\n",
    "    # count of \"features\" #\n",
    "    train_df[\"num_features\"], test_df[\"num_features\"] = train_df[\"features\"].apply(len), test_df[\"features\"].apply(len)\n",
    "\n",
    "    # count of words present in description column #\n",
    "    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    \n",
    "    tmp = train_df.append(test_df)\n",
    "    gp = tmp.groupby('manager_id').size()\n",
    "    gp.name = 'manager_count'\n",
    "    gp = gp.reset_index()\n",
    "    train_df, test_df = pd.merge(train_df,gp,how='left'), pd.merge(test_df,gp,how='left')\n",
    "    \n",
    "    gp = tmp.groupby('building_id').size()\n",
    "    gp.name = 'building_count'\n",
    "    gp = gp.reset_index()\n",
    "    train_df, test_df = pd.merge(train_df,gp,how='left'), pd.merge(test_df,gp,how='left')\n",
    "    \n",
    "    train_df['zero_building_id'] = train_df['building_id'].map(lambda x:int(x=='0'))\n",
    "    test_df['zero_building_id'] = test_df['building_id'].map(lambda x:int(x=='0'))\n",
    "    fea_list = ['hour_size','zero_building_id','weekdays',\"num_features\",\"num_description_words\",\n",
    "                \"days\",\"num_photos\", \"created_month\", \"created_day\", \"created_hour\",'manager_count','building_count']\n",
    "    \n",
    "    return train_df,test_df,fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residue_fea(train_df,test_df):\n",
    "    if 'days' not in train_df.columns:\n",
    "        train_df[\"created\"], test_df[\"created\"] = pd.to_datetime(train_df[\"created\"]), pd.to_datetime(test_df[\"created\"])\n",
    "        test_df['days'], train_df['days'] = test_df['created']-pd.to_datetime('2016-04-01'), train_df['created'] - pd.to_datetime('2016-04-01')\n",
    "        \n",
    "    train_df['dayx20'] = train_df['days'].map(lambda x:int(x*20))\n",
    "    gp = train_df.groupby('dayx20',as_index=False)['listing_id'].min()\n",
    "    gp['dayx20'] = gp['dayx20']/20.\n",
    "    \n",
    "    lin = LinearRegression()\n",
    "    X = gp['dayx20'].as_matrix().reshape(len(gp),1)\n",
    "    y = np.array(gp['listing_id'])\n",
    "    lin.fit(X,y)\n",
    "    \n",
    "    train_df['listing_id_residue'] = train_df['days'].map(lambda x:x*lin.coef_[0] + lin.intercept_)\n",
    "    train_df['listing_id_residue'] = train_df['listing_id'] - train_df['listing_id_residue'] \n",
    "    test_df['listing_id_residue'] = test_df['days'].map(lambda x:x*lin.coef_[0] + lin.intercept_)\n",
    "    test_df['listing_id_residue'] = test_df['listing_id'] - train_df['listing_id_residue'] \n",
    "    \n",
    "    train_df['int_days'] = train_df['days'].map(lambda x:int(x))\n",
    "    train_df['intra'] = train_df['days'] - train_df['int_days']\n",
    "    train_df['intra'] = train_df['intra'].map(lambda x:int(x>0.3))\n",
    "    \n",
    "    test_df['int_days'] = test_df['days'].map(lambda x:int(x))\n",
    "    test_df['intra'] = test_df['days'] - train_df['int_days']\n",
    "    test_df['intra'] = test_df['intra'].map(lambda x:int(x>0.3))\n",
    "    del train_df['dayx20']\n",
    "    return train_df,test_df,['int_days','intra','listing_id_residue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def careful_avg(train_df_,test_df_,agg_keys,mask,suffix,debug=False):\n",
    "    \"\"\"\n",
    "    agg_keys: the list of column names groupby is based on, e.g.['manager_id','bedrooms','bathrooms']\n",
    "    mask has same length of train_df, used for cross validation\n",
    "    mask=1 training, masking=0 validation\n",
    "    suffix is for the resulted feature names\n",
    "    \"\"\"\n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    print train_df.shape\n",
    "    print test_df.shape\n",
    "    \n",
    "    train_df['target'] = train_df['interest_level']\n",
    "    test_df['target'] = -1\n",
    "    \n",
    "    train_df.loc[train_df['mask']==0,'target'] = -1\n",
    "    alldf = train_df.append(test_df)\n",
    "    alldf['target'] = alldf['target'].fillna(-1)\n",
    "    alldf['mask'] = alldf['mask'].fillna(0)\n",
    "    \n",
    "    if debug:\n",
    "        debug_list =  set(['401 w 25th st.','30 west 63rd street'])\n",
    "        alldf = alldf[alldf['street_address'].isin(debug_list)]\n",
    "        import pdb;pdb.set_trace()\n",
    "    \n",
    "    global_avg = train_df[train_df['mask']==1].groupby('target').count()*1.0/np.sum(mask)\n",
    "    columns = ['#records_'+suffix,'#train_record_'+suffix,'mean_price_apt_'+suffix,'low_avg_'+suffix,\n",
    "               'med_avg_'+suffix,'high_avg_'+suffix]\n",
    "    low_avg_name = 'low_avg_'+suffix\n",
    "    med_avg_name = 'med_avg_'+suffix\n",
    "    high_avg_name = 'high_avg_'+suffix\n",
    "    gp = alldf.groupby(agg_keys)\n",
    "    res = gp['mask'].agg({'#train_record_'+suffix:np.sum,\n",
    "                         '#records_'+suffix:np.size})\n",
    "    res['mean_price_apt_'+suffix] = gp['price'].mean()\n",
    "    res2 = gp['target'].agg({low_avg_name:lambda x:np.sum(x==0)*1.,\n",
    "                            med_avg_name:lambda x:np.sum(x==1)*1.,\n",
    "                            high_avg_name:lambda x:np.sum(x==2)*1.,\n",
    "                            'total':lambda x:np.sum(x!=-1)})\n",
    "    res = res.join(res2)\n",
    "    res = res.reset_index()\n",
    "    res = pd.merge(alldf[['listing_id','mask','target'] + agg_keys],res,on=agg_keys,how='left')\n",
    "    res.loc[res['target']==0,low_avg_name] = res.loc[res['target']==0,low_avg_name] - 1\n",
    "    res.loc[res['target']==1,med_avg_name] = res.loc[res['target']==1,med_avg_name] - 1\n",
    "    res.loc[res['target']==2,high_avg_name] = res.loc[res['target']==2,high_avg_name] - 1\n",
    "    res.loc[res['target']>=0,'total'] = res.loc[res.target>=0,'total'] - 1\n",
    "    res.loc[res['total']>0,low_avg_name] = res.loc[res['total']>0,low_avg_name]/res.loc[res['total']>0,'total']\n",
    "    res.loc[res['total']>0,med_avg_name] = res.loc[res['total']>0,med_avg_name]/res.loc[res['total']>0,'total']\n",
    "    res.loc[res['total']>0,high_avg_name] = res.loc[res['total']>0,high_avg_name]/res.loc[res['total']>0,'total']\n",
    "    res.loc[res['total']==0,low_avg_name] = res.loc[res['total']>0,low_avg_name].mean()\n",
    "    res.loc[res['total']==0,med_avg_name] = res.loc[res['total']>0,med_avg_name].mean()\n",
    "    res.loc[res['total']==0,high_avg_name] = res.loc[res['total']>0,high_avg_name].mean()\n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "        res.to_csv('debug_avg.csv',index=False,encoding='utf8')\n",
    "        raise NameError('Debug exit')\n",
    "    train_df = pd.merge(train_df,res[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    test_df = pd.merge(test_df,res[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    \n",
    "    del train_df['target']\n",
    "    \n",
    "    del test_df['target']\n",
    "    \n",
    "    return train_df,test_df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def past_future(train_df_,test_df_,agg_keys,mask,suffix,debug=False):\n",
    "    \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    test_df['target'] = -1\n",
    "    \n",
    "    train_df['target'] = train_df['interest_level']\n",
    "    train_df.loc[train_df['mask']==0,'target'] = -1\n",
    "    alldf = train_df.append(test_df)\n",
    "    alldf['target'] = alldf['target'].fillna(-1)\n",
    "    alldf['mask'] = alldf['mask'].fillna(0)\n",
    "    if debug:\n",
    "        debug_list =  set(['401 w 25th st.','30 west 63rd street'])\n",
    "        alldf = alldf[alldf['street_address'].isin(debug_list)]\n",
    "        import pdb;pdb.set_trace()\n",
    "    if 'days' not in alldf.columns:\n",
    "        raise NameError(\"This function should be called after basic_feature()\")\n",
    "    alldf = alldf.sort_values('days')\n",
    "    alldf.index=range(len(alldf))\n",
    "    #gp = alldf.groupby(['manager_id','building_id','bathrooms','bedrooms','street_address'])\n",
    "    gp = alldf.groupby(agg_keys)\n",
    "    alldf['last_listing_day_'+suffix] = gp['days'].transform(lambda x:x.diff(1)).fillna(0)    \n",
    "    alldf['next_listing_days_'+suffix] = gp['days'].transform(lambda x:x.diff(-1)).fillna(0)    \n",
    "    alldf['last_listing_price_'+suffix] = gp['price'].transform(lambda x:x.diff(1)).fillna(0)\n",
    "    alldf['next_listing_price_'+suffix] = gp['price'].transform(lambda x:x.diff(-1)).fillna(0)\n",
    "    \n",
    "    alldf['last_listing_interest_'+suffix] = gp['target'].transform(lambda x:x.shift(1)).fillna(-1)\n",
    "    alldf['next_listing_interest_'+suffix] = gp['target'].transform(lambda x:x.shift(-1)).fillna(-1)\n",
    "    \n",
    "    if debug:\n",
    "        alldf.to_csv('debug.csv',index=False,encoding='utf8')\n",
    "        raise NameError('Exit by Debug')\n",
    "        \n",
    "    columns = ['last_listing_day_'+suffix,'next_listing_days_'+suffix,'last_listing_price_'+suffix,\n",
    "              'next_listing_price_'+suffix,'last_listing_interest_'+suffix,'next_listing_interest_'+suffix]\n",
    "    \n",
    "    train_df = pd.merge(train_df,alldf[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    test_df = pd.merge(test_df,alldf[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    \n",
    "    del train_df['target']\n",
    "    \n",
    "    del test_df['target']\n",
    "    return train_df,test_df,columns\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB_sklearn(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=10, num_rounds=5000,verbose=False):\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=num_rounds,\n",
    "                            objective='multi:softprob',\n",
    "                            learning_rate=0.003,\n",
    "                            max_depth=6,\n",
    "                            min_child_weight=1,\n",
    "                            subsample=.7,\n",
    "                            colsample_bytree=.7,\n",
    "                            colsample_bylevel=.5,\n",
    "                            gamma=0.005,\n",
    "                            scale_pos_weight=1,\n",
    "                            base_score=.5,\n",
    "                            #reg_lambda=0,\n",
    "                            #reg_alpha=0,\n",
    "                            #missing=0,\n",
    "                            seed=seed_val)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        clf.fit(train_X, train_y,eval_set=[(train_X, train_y), (test_X, test_y)],verbose=verbose,eval_metric='mlogloss',\n",
    "            early_stopping_rounds=50)\n",
    "    else:        \n",
    "        clf.fit(train_X, train_y,verbose=False)\n",
    "    pred_test_y = clf.predict_proba(test_X)\n",
    "    return pred_test_y, clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_cv(train_df,test_df,fealist):\n",
    "    train_X = train_df[fealist].as_matrix()\n",
    "    test_X = test_df[fealist].as_matrix()\n",
    "    train_y = np.array(train_df['interest_level'])\n",
    "    \n",
    "    cv_scores = [] \n",
    "    #print fea_categorical\n",
    "    #print fea_additional\n",
    "    #print feature_params\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=22)\n",
    "    for dev_index, val_index in kf.split(train_X,train_y):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB_sklearn(dev_X, dev_y, val_X, val_y,verbose=False)\n",
    "        print('best iterations:{}, best_score={}, last_score={}'.format(model.best_iteration,\n",
    "                                                                   model.best_score,log_loss(val_y, preds)))\n",
    "        importance_inx = np.argsort(model.feature_importances_*-1)\n",
    "        print('Most important 40 features:')\n",
    "        ff = [(fealist[x],model.feature_importances_[x]) for x in importance_inx[:40]]\n",
    "        print(ff)\n",
    "        print('-------------------------')\n",
    "          \n",
    "    \n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "    print 'mean score={}'.format(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddrBuildingAggregate(BaseFeatureExtraction):\n",
    "    def __init__(self):\n",
    "        super(AddrBuildingAggregate,self).__init__()\n",
    "        self.method_info = \"aggregation based on building or street address, take the price difference\"\n",
    "        \n",
    "    \n",
    "\n",
    "    def transform(self,train_df_,test_df_):    \n",
    "        if 'west_east' not in train_df_.columns:\n",
    "            raise NameError('This function should be called after the AddressFeature')\n",
    "        df1 = train_df_.copy()\n",
    "        df2 = test_df_.copy()\n",
    "        df1['source'] = 1\n",
    "        df2['source'] = 2\n",
    "        df = df1.append(df2)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        def local_agg(df,agg_keys,fea_name):            \n",
    "            p = df.groupby(agg_keys)['price'].mean()\n",
    "            p.name = fea_name\n",
    "            p = p.reset_index()\n",
    "            df = pd.merge(df,p,how='left',on=agg_keys)\n",
    "            df[fea_name] = df['price']-df[fea_name]\n",
    "            return df\n",
    "        \n",
    "        local_agg(df,['building_id','bathrooms','bedrooms'],'building_price')\n",
    "        local_agg(df,['building_id','bedrooms'],'building_bed_price')\n",
    "        local_agg(df,['street_address','bathrooms','bedrooms'],'stradd_price')\n",
    "        local_agg(df,['street_address','bedrooms'],'stradd_bed_price')\n",
    "        \n",
    "        df1 = df[df['source']==1].copy()\n",
    "        df2 = df[df['source']==2].copy()\n",
    "        del df1['source']\n",
    "        del df2['source']\n",
    "        del df\n",
    "    \n",
    "        return df1,df2,['building_price','building_bed_price','stradd_price','stradd_bed_price']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BayesCategoricalDistribution(train_df_,test_df_,agg_keys,mask,suffix,multiple=5.):\n",
    "    \n",
    "    if len(mask)!=len(train_df_):\n",
    "        raise ValueError(\"Length of mask should be equal to length of train_df\")\n",
    "    \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()    \n",
    "    train_df['target'] = train_df['interest_level']\n",
    "    test_df['target'] = -1\n",
    "    test_df['mask'] = 0\n",
    "    #train_df['mask'] = mask    \n",
    "    train_df.loc[train_df['mask']==0,'target'] = -1\n",
    "    alldf = train_df.append(test_df)\n",
    "    \n",
    "    gp = alldf.groupby(agg_keys).size()\n",
    "    gp.name = 'size'\n",
    "    gp = gp.reset_index()\n",
    "    alldf = pd.merge(alldf,gp,how='left')\n",
    "    \n",
    "    tmpdf = alldf.loc[alldf['mask']==1,['size','target']].copy()\n",
    "    tmpdf['low_global'] = tmpdf['target'].map(lambda x:int(x==0))\n",
    "    tmpdf['medium_global'] = tmpdf['target'].map(lambda x:int(x==1))\n",
    "    tmpdf['high_global'] = tmpdf['target'].map(lambda x:int(x==2))\n",
    "    tmpdf['total_global'] = 1.\n",
    "    tmpdf = tmpdf.groupby('size').sum()\n",
    "    tmpdf['low_global'] = tmpdf['low_global']/tmpdf['total_global']\n",
    "    tmpdf['medium_global'] = tmpdf['medium_global']/tmpdf['total_global']\n",
    "    tmpdf['high_global'] = tmpdf['high_global']/tmpdf['total_global']\n",
    "    tmpdf = tmpdf.reset_index()\n",
    "    del tmpdf['target']\n",
    "    import pdb;pdb.set_trace()\n",
    "    \n",
    "    alldf = pd.merge(alldf,tmpdf,how='left',on='size')\n",
    "    columns = ['#train_record_'+suffix,'low_avg_'+suffix,'med_avg_'+suffix,'high_avg_'+suffix]\n",
    "    low_avg_name = 'low_avg_'+suffix\n",
    "    med_avg_name = 'med_avg_'+suffix\n",
    "    high_avg_name = 'high_avg_'+suffix\n",
    "    \n",
    "    gp = alldf.groupby(agg_keys)\n",
    "    res = gp['target'].agg({low_avg_name:lambda x:np.sum(x==0)*1.,\n",
    "                            med_avg_name:lambda x:np.sum(x==1)*1.,\n",
    "                            high_avg_name:lambda x:np.sum(x==2)*1.,\n",
    "                            'total':lambda x:np.sum(x!=-1)})\n",
    "    res2 = gp['mask'].agg({'#train_record_'+suffix:np.sum,})\n",
    "    res = res.join(res2)\n",
    "    res = res.reset_index()\n",
    "    \n",
    "    res = pd.merge(alldf[['listing_id','mask','target','low_global','medium_global','high_global'] + agg_keys],\n",
    "                   res,on=agg_keys,how='left')\n",
    "    res.loc[res['target']==0,low_avg_name] = res.loc[res['target']==0,low_avg_name] - 1\n",
    "    res.loc[res['target']==1,med_avg_name] = res.loc[res['target']==1,med_avg_name] - 1\n",
    "    res.loc[res['target']==2,high_avg_name] = res.loc[res['target']==2,high_avg_name] - 1\n",
    "    res.loc[res['target']>=0,'total'] = res.loc[res.target>=0,'total'] - 1\n",
    "    res[low_avg_name] = (res[low_avg_name] + res['low_global']*multiple)/(res['total']+multiple)\n",
    "    res[med_avg_name] = (res[med_avg_name] + res['medium_global']*multiple)/(res['total']+multiple)\n",
    "    res[high_avg_name] = (res[high_avg_name] + res['high_global']*multiple)/(res['total']+multiple)\n",
    "    \n",
    "    train_df = pd.merge(train_df,res[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    test_df = pd.merge(test_df,res[columns+['listing_id']],how='left',on=['listing_id'])\n",
    "    del train_df['target']\n",
    "    del test_df['target']\n",
    "    del train_df['mask']\n",
    "    del test_df['mask']\n",
    "    \n",
    "    return train_df,test_df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_col = ['created','listing_id','price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_fea = TextFeature()\n",
    "mis_fea = Miscellous()\n",
    "addr_fea = AddressFeature()\n",
    "gbm_quant_fea = GbmQuantPrice(['days','latitude','longitude'],'gbm_quant_lat_long')\n",
    "addr_aggr_fea = AddrBuildingAggregate()\n",
    "cat_encoding_fea = CategoricalEncoding()\n",
    "cat_cv_fea = Categorical_cv(nfold = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "core_feature = ['bathrooms','bedrooms','latitude','longitude','price','listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    created  listing_id  price\n",
      "10      2016-06-24 07:54:24     7211212   3000\n",
      "10000   2016-06-12 12:19:27     7150865   5465\n",
      "100004  2016-04-17 03:26:41     6887163   2850\n",
      "100007  2016-04-18 02:22:02     6888711   3275\n",
      "100013  2016-04-28 01:32:41     6934781   3350\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df = read_data()\n",
    "print train_df[print_col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df,addr_fealist = addr_fea.transform(train_df,test_df)\n",
    "train_df,test_df,basic_fealist = mis_fea.transform(train_df,test_df)\n",
    "train_df,test_df,fealist_txt = txt_fea.transform(train_df,test_df)\n",
    "train_df,test_df,residue_fealist = residue_fea(train_df,test_df)\n",
    "train_df,test_df,gbm_quant_fealist = gbm_quant_fea.transform(train_df,test_df)\n",
    "train_df,test_df,addr_aggr_fealist = addr_aggr_fea.transform(train_df,test_df)\n",
    "train_df,test_df,cate_encoding_fealist = cat_encoding_fea.transform(train_df,test_df)\n",
    "print train_df[print_col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-27-503abeab8235>(33)BayesCategoricalDistribution()\n",
      "-> alldf = pd.merge(alldf,tmpdf,how='left',on='size')\n",
      "(Pdb) c\n",
      "              created  listing_id  price\n",
      "0 2016-06-24 07:54:24     7211212   3000\n",
      "1 2016-06-12 12:19:27     7150865   5465\n",
      "2 2016-04-17 03:26:41     6887163   2850\n",
      "3 2016-04-18 02:22:02     6888711   3275\n",
      "4 2016-04-28 01:32:41     6934781   3350\n",
      "more -----\n",
      "(39481, 316)\n",
      "best iterations:3454, best_score=0.553947, last_score=0.554042467929\n",
      "0.554042467929\n",
      "> <ipython-input-27-503abeab8235>(33)BayesCategoricalDistribution()\n",
      "-> alldf = pd.merge(alldf,tmpdf,how='left',on='size')\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-21ec5cda307a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     train_df,test_df,fea_avg_manager = BayesCategoricalDistribution(train_df,test_df,agg_keys=['manager_id'],\n\u001b[0;32m---> 38\u001b[0;31m                                        mask=mask,suffix='manager')\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprint_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-503abeab8235>\u001b[0m in \u001b[0;36mBayesCategoricalDistribution\u001b[0;34m(train_df_, test_df_, agg_keys, mask, suffix, multiple)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0malldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmpdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'#train_record_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'low_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'med_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'high_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlow_avg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'low_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-503abeab8235>\u001b[0m in \u001b[0;36mBayesCategoricalDistribution\u001b[0;34m(train_df_, test_df_, agg_keys, mask, suffix, multiple)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0malldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmpdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'#train_record_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'low_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'med_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'high_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlow_avg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'low_avg_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df_original = train_df.copy()\n",
    "test_df_original = test_df.copy()\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=22)\n",
    "train_df_original.index=range(len(train_df))\n",
    "train_y = train_df_original['interest_level'].as_matrix().ravel()\n",
    "ii = 1\n",
    "cv_scores=[]\n",
    "core_feature = ['bathrooms','bedrooms','latitude','longitude','price','listing_id']\n",
    "print_col = ['created','listing_id','price']\n",
    "for dev_index,val_index in kf.split(train_df[['listing_id','price']].as_matrix(),train_y):\n",
    "    \n",
    "    mask = np.ones(len(train_df))\n",
    "    mask[val_index] = 0\n",
    "    train_df_original['mask'] = mask\n",
    "    test_df_original['mask'] = 0\n",
    "    #fea_past_fut_manager_building = []\n",
    "    #train_df,test_df,fea_past_fut_manager_building = past_future(train_df_original,test_df_original,\n",
    "    #                                agg_keys=['manager_id','building_id','bathrooms','bedrooms','street_address'],\n",
    "    #                                mask=mask,suffix='manager_building')\n",
    "    #print train_df[print_col].head(5)\n",
    "    \n",
    "    #train_df,test_df,fea_avg_building_bed_bath = careful_avg(train_df,test_df,\n",
    "    #                                agg_keys=['building_id','street_address','bedrooms','bathrooms'],\n",
    "    #                                mask=mask,suffix='building_bed_bath')    \n",
    "    #print train_df_[print_col].head(5)\n",
    "    #train_df,test_df,fea_avg_building = careful_avg(train_df,test_df,agg_keys=['building_id'],\n",
    "    #                                   mask=mask,suffix='building')\n",
    "    #print train_df[print_col].head(5)\n",
    "    #nmd = train_df.copy()\n",
    "    #del nmd['description']\n",
    "    #del nmd['features']\n",
    "    #del nmd['photos']\n",
    "    #nmd.to_csv('nmd.csv',index=False,encoding='utf8')\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    train_df,test_df,fea_avg_manager = BayesCategoricalDistribution(train_df,test_df,agg_keys=['manager_id'],\n",
    "                                       mask=mask,suffix='manager')\n",
    "    print train_df[print_col].head(5)\n",
    "    \n",
    "    #train_df,test_df,fea_past_fut_building = past_future(train_df,test_df,\n",
    "    #                                agg_keys=['building_id','bathrooms','bedrooms','street_address'],\n",
    "    #                                mask=mask,suffix='building')\n",
    "    #print train_df[print_col].head(5)\n",
    "\n",
    "    #fealist = core_feature + basic_fealist + residue_fealist + fea_avg_building_bed_bath + fea_avg_building + fea_avg_manager + fea_past_fut_manager_building + fea_past_fut_building\n",
    "    #fealist = core_feature + basic_fealist + residue_fealist  + fea_avg_building + fea_avg_manager + fea_past_fut_manager_building \n",
    "    fealist0 = core_feature + basic_fealist + fealist_txt + residue_fealist + addr_fealist + gbm_quant_fealist + addr_aggr_fealist + cate_encoding_fealist\n",
    "    fealist1 = fealist0 + fea_avg_manager\n",
    "    def runone(train_df,train_y,mask,fealist):\n",
    "        X_train = train_df.loc[mask==1,fealist]\n",
    "        print X_train.shape\n",
    "        y_train = train_y[mask==1]\n",
    "        X_test = train_df.loc[mask==0,fealist]\n",
    "        y_test = train_y[mask==0]\n",
    "        preds,model = runXGB_sklearn(X_train,y_train,X_test,y_test)\n",
    "        print('best iterations:{}, best_score={}, last_score={}'.format(model.best_iteration,\n",
    "                                                                   model.best_score,log_loss(y_test, preds)))\n",
    "        print log_loss(y_test,preds)\n",
    "    \n",
    "    #print('best iterations:{}, best_score={}, last_score={}'.format(model.best_iteration,\n",
    "#                                                                   model.best_score,log_loss(y_test, preds)))\n",
    "    #importance_inx = np.argsort(model.feature_importances_*-1)\n",
    "    #print('Most important 40 features:')\n",
    "    #ff = [(fealist[x],model.feature_importances_[x]) for x in importance_inx[:40]]\n",
    "    #print(ff)    \n",
    "    #cv_scores.append(log_loss(y_test,preds)); print(cv_scores)\n",
    "    #print 'basic ---'\n",
    "    #runone(train_df,train_y,mask,fealist0)\n",
    "    \n",
    "    print 'more -----'\n",
    "    runone(train_df,train_y,mask,fealist1)\n",
    "    \n",
    "    \n",
    "print 'mean score={}'.format(np.mean(cv_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = train_df.groupby('manager_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 21)\n",
    "0.555284040866\n",
    "more -----\n",
    "(39481, 27)\n",
    "0.553684755985\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 21)\n",
    "0.563221004316\n",
    "more -----\n",
    "(39481, 27)\n",
    "0.561319460851\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 21)\n",
    "0.554102758386\n",
    "more -----\n",
    "(39481, 27)\n",
    "0.553038442705\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39481, 21)\n",
    "0.559244335347\n",
    "more -----\n",
    "(39481, 27)\n",
    "0.557624528681\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "              created  listing_id  price\n",
    "0 2016-06-24 07:54:24     7211212   3000\n",
    "1 2016-06-12 12:19:27     7150865   5465\n",
    "2 2016-04-17 03:26:41     6887163   2850\n",
    "3 2016-04-18 02:22:02     6888711   3275\n",
    "4 2016-04-28 01:32:41     6934781   3350\n",
    "basic ---\n",
    "(39484, 21)\n",
    "0.561456910202\n",
    "more -----\n",
    "(39484, 27)\n",
    "0.560915910552\n",
    "mean score=nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
