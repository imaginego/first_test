{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20,verbose_eval=False)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldf = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "manager_id\n",
       "d3b4ad3b4de64390657f4be37935c435     197\n",
       "2983e45f7e0ad87d677dacd13e362785     202\n",
       "324631e1aacaa41bc4475f879756bbb6     204\n",
       "5546ad6e8ea7f292696bec3c1c0ccb2d     205\n",
       "6b1ee996b1c87cd54c6cd92acbef9aac     205\n",
       "7c5e4fc025b70c6540d6b0e06716b9dd     206\n",
       "9ba3641676cc717a8b65d59dac8545fb     206\n",
       "39af186286605963b1d75543e1492c61     207\n",
       "c1a6598437b7db560cde66e5a297a53f     207\n",
       "c671e697434aa710888b0632eb2c6145     209\n",
       "964dc31a872efa33fee9af11f62e843c     211\n",
       "0f57223e3bbd5222d9881a442aba0d03     212\n",
       "a21c0ea7f452539ee52293a199e40236     213\n",
       "e0f4c69279ce5ae6421b39d46303554b     214\n",
       "06ba79af09fa392b303d82c65318e94a     217\n",
       "26d69329a0f3b9abecbddd6abcb5b622     218\n",
       "cb8c110c47bf709e413ed0a0ac40dc09     219\n",
       "d4e72926d837c6ca5849c62cead367b4     221\n",
       "34545e3e23ce49e85d161ff75309d0f6     221\n",
       "5a002b00880a84406292a8dfe54f8c72     222\n",
       "0a6ba20b892ed61df751420934d1ab51     223\n",
       "48da4647746078e946b008a561123653     223\n",
       "045f9f5708f6896535892b85a81d8151     226\n",
       "babf967aeb47132007ac2dd76c204d49     227\n",
       "536aaedf27d13fb487c142dae8133211     228\n",
       "5c7ca9f5239f481c0f35f3faaff188bd     231\n",
       "12c0a30e296faa0dfd422fe918d7d4f4     233\n",
       "92aa3b535f48ec05903a3b7fcdafd411     233\n",
       "ab8c9250b384ccd4867aac5498d9a78f     234\n",
       "5ba989232d0489da1b5f2c45f6688adc     235\n",
       "                                    ... \n",
       "781c05e464ce94eaa436401d07f2a6cc     383\n",
       "c71cf1f472cf9b4b4517ea23fb6f2c91     391\n",
       "699c325b818541f314b691b76f3238d7     404\n",
       "501b52559fd21b32808030a0f4fb1a26     425\n",
       "381040570cc863b743a9fce1a6ae9724     426\n",
       "d399821d8583b8421a55370c6eb15f6f     430\n",
       "eb5a6c4c2eb6f3e2a085c477afd5a815     441\n",
       "1067e078446a7897d2da493d2f741316     470\n",
       "8b53ccf4338806ab1be3dd0267711649     482\n",
       "fc81b75568d3655a922523cb0d77ea5b     505\n",
       "d1737922fe92ccb0dc37ba85589e6415     521\n",
       "8262449f40e9117f7a9ea49b4a333993     522\n",
       "612a00076aefe8c98d1df4835640c74b     523\n",
       "b209e2c4384a64cc307c26759ee0c651     538\n",
       "dbbb6b990661b1e507a387f019bcb1a0     552\n",
       "62826f3ae01f2ddc93b9cd28c659ab2b     585\n",
       "aa9e353a6b43b125cbc89cb751090a9e     643\n",
       "d2bce61e0e0079ebdc8c281e415e045b     677\n",
       "5599e962719af3ccc2976855c2d5893c     713\n",
       "1fb46c4a72bcf764ac35fc23f394760d     719\n",
       "c9c33695ee2a2f818e9f1d8f7d1c4b39     731\n",
       "ad3d8ddc52c7e0859b5c6c7f7949c3bd     732\n",
       "b7de4cb395920136663132057fa89d84     849\n",
       "2aa9bfa5f67ed9997ea341dee8a3a271     853\n",
       "9df32cb8dda19d3222d66e69e258616b     862\n",
       "62b685cc0d876c3a1a51d63a0d6a8082     956\n",
       "cb87dadbca78fad02b388dc9e8f25a5b     971\n",
       "8f5a9c893f6d602f4953fcc0b8e6e9b4    1011\n",
       "6e5c10246156ae5bdcd9b487ca99d96a    1683\n",
       "e6472c7237327dd3903b3d6f6a94515a    6387\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldf.groupby('manager_id').size().sort_values().tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",'listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "test_df['days'] = test_df['created']-pd.to_datetime('2016-04-01')\n",
    "train_df['days'] = train_df['created'] - pd.to_datetime('2016-04-01')\n",
    "\n",
    "test_df['days'] = test_df['days']/np.timedelta64(1, 'D')\n",
    "train_df['days'] = train_df['days']/np.timedelta64(1,'D')\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"days\",\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])\n",
    "#features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"days\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10                                                         \n",
      "10000     Doorman Elevator Fitness_Center Cats_Allowed D...\n",
      "100004    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
      "100007                               Hardwood_Floors No_Fee\n",
      "100013                                              Pre-War\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((49352, 214), (74659, 214))\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.551690243641764]\n",
      "[0.551690243641764, 0.55294574405869701]\n",
      "[0.551690243641764, 0.55294574405869701, 0.55520924622355272]\n",
      "[0.551690243641764, 0.55294574405869701, 0.55520924622355272, 0.54228432263018145]\n",
      "[0.551690243641764, 0.55294574405869701, 0.55520924622355272, 0.54228432263018145, 0.54474571411498374]\n",
      "0.549375054134\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
