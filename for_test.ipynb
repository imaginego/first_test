{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianqi/anaconda2/envs/xgboost/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import hashlib\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Column 'features' -- cleaning, transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_fea(df1_,df2_,nTop=300,combine=True):\n",
    "    ''' df1_ is training set\n",
    "        df2_ is test set\n",
    "    '''\n",
    "    from collections import defaultdict\n",
    "    df1 = df1_.copy() # Maybe ther eis better way to avoid SettingWithCopyWarning \n",
    "    df2 = df2_.copy()\n",
    "    df1['source'] = 1\n",
    "    df2['source'] = 2\n",
    "    df = df1.append(df2)\n",
    "    df = df.reset_index(drop=True)\n",
    "    #df['features'] = df['features'].map(lambda x:[tt.lower() for tt in x])\n",
    "    df['description'] = df['description'].map(lambda x:x.lower())\n",
    "    #---------------------------\n",
    "    #original length of the first feature, meant to capture those features typed in with wrong \n",
    "    # format -- all features are cramed into one phrase\n",
    "    df['len_feature0'] = df['features'].map(lambda x:0 if len(x)==0 else len(x[0])) \n",
    "    #-----------------------------\n",
    "    \n",
    "    def fea_clean(x): \n",
    "        if len(x) == 1:\n",
    "            tmp = x[0].strip('*').split('*')\n",
    "            if len(tmp) ==1:\n",
    "                tmp = tmp[0].split(u'\\u2022')\n",
    "            x = tmp\n",
    "        ret = [tt.encode('utf-8').decode('unicode_escape').encode('ascii','ignore').lower().strip() for tt in x]\n",
    "        return ret\n",
    "    df['features'] = df['features'].map(fea_clean)\n",
    "    \n",
    "    \n",
    "    all_fea = defaultdict(int)\n",
    "    for _,row in df.iterrows():\n",
    "        for xx in row['features']:\n",
    "            all_fea[xx] += 1\n",
    "    sorted_fea = sorted(all_fea.iteritems(),key=lambda (k,v): v,reverse=True)\n",
    "    \n",
    "    combined_fea = {'laundry in unit':['laundry in unit','in-unit washer/dryer','washer & dryer',\n",
    "                                        'washer/dryer','washer/dryer in unit'],\n",
    "                   'laundry in building':['laundry in building','laundry room',\n",
    "                                           'washer/dryer in building','on-site laundry'],\n",
    "                   'gym/fitness':['gym/fitness','fitness center','gym','gym in building'],\n",
    "                   'pre-war':['pre-war','prewar'],\n",
    "                    'live-in superintendent':['live-in superintendent','live-in super','live in super'],\n",
    "                    'hardwood floors':['hardwood floors','hardwood','hardwood floor','hardwood flooring'],\n",
    "                    'high ceiling':['high ceiling','high ceilings'],\n",
    "                    'full-time doorman':['full-time doorman','ft doorman','24/7 doorman','24 hour doorman',\n",
    "                                        '24-hour doorman','24hr doorman','full time doorman','24 hr doorman']\n",
    "                   #'garage':['garage','parking']\n",
    "                   }\n",
    "    \n",
    "    fea_list = set([v[0] for v in sorted_fea[:nTop]])\n",
    "    for k,v in combined_fea.iteritems():\n",
    "        fea_list = fea_list.union(set(v))\n",
    "    for fea in fea_list:\n",
    "        df[fea] = 0\n",
    "    #import pdb;pdb.set_trace()\n",
    "    for inx,row in df.iterrows():\n",
    "        notlist = []\n",
    "        if len(row['features']) == 0:\n",
    "            continue\n",
    "        for ff in row['features']:\n",
    "            if ff in fea_list:\n",
    "                df.set_value(inx,ff,1)\n",
    "            else:\n",
    "                notlist.append(ff)\n",
    "        #df.set_value(inx,'description',row['description'] + ' '.join(notlist))\n",
    "    print 'fea_list length is {}'.format(len(fea_list))\n",
    "    fea_list = set(fea_list)\n",
    "    if combine:        \n",
    "        for k,v in combined_fea.iteritems():\n",
    "            df[k] = df[v[0]]\n",
    "            #print k,v\n",
    "            for ii in range(1,len(v)):\n",
    "                df[k] = df[k] + df[v[ii]]\n",
    "                del df[v[ii]]\n",
    "                fea_list.remove(v[ii])\n",
    "    fea_list = list(fea_list) + ['len_feature0']\n",
    "    #print 'fea_list length is {}'.format(len(fea_list))\n",
    "    df1 = df[df['source']==1].copy()\n",
    "    df2 = df[df['source']==2].copy()\n",
    "    del df1['source']\n",
    "    del df2['source']\n",
    "    del df\n",
    "    return df1,df2,fea_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street_address and display address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_num(x):\n",
    "    if x[-1]=='1':\n",
    "        return x+'st'\n",
    "    elif x[-1]=='2':\n",
    "        return x+'nd'\n",
    "    elif x[-1]=='3':\n",
    "        return x+'rd'\n",
    "    else:\n",
    "        return x+'th'\n",
    "\n",
    "def normalize_street(x):\n",
    "    street_name_mapping = {'st.':'street','st':'street','st,':'street','st..':'street','street,':'street',\n",
    "                       'ave':'avenue','ave.':'avenue','ave,':'avenue','avenue,':'avenue','pl':'place',\n",
    "                       'blvd':'boulevard','pkwy':'parkway','dr':'drive','rd.':'road','rd,':'road','rd':'road',\n",
    "                       'ln':'lane',\n",
    "                       'e':'east','e.':'east','w.':'west','w':'west','west,':'west','s':'south','&':'and',\n",
    "                       'second':'2nd','first':'1st','third':'3rd','fourth':'4th','fifth':'5th',\n",
    "                       'sixth':'6th','seventh':'7th','eighth':'8th','ninth':'9th','tenth':'10th',                       \n",
    "                       #'1':'1st','2':'2nd','43':'43rd','37':'37th','34':'34th',\n",
    "                      }\n",
    "    xlist = x.lower().strip(' .,').split()\n",
    "    output = []\n",
    "    for tt in xlist:\n",
    "        tmp = tt.strip(',.*')\n",
    "        if len(tmp)>0:\n",
    "            if tmp in street_name_mapping:\n",
    "                tmp = street_name_mapping[tmp]\n",
    "            elif tmp.isdigit():\n",
    "                tmp = normalize_num(tmp)\n",
    "            else:\n",
    "                pass\n",
    "            output.append(tmp)\n",
    "    return ' '.join(output).strip()      \n",
    "\n",
    "def rem_streetname_xy(x,y):\n",
    "    '''remove x from y\n",
    "    '''\n",
    "    pos = y.find(x)\n",
    "    if pos>0:\n",
    "        tmp = y.replace(x,'').strip(' ,.')\n",
    "        tmp = tmp.split(' ')[0].strip(',. #')\n",
    "        if '-' in tmp:        \n",
    "            tmp = tmp.split('-')[0].strip(', .#')\n",
    "        try:\n",
    "            a = int(tmp)\n",
    "            return a\n",
    "        except ValueError:\n",
    "            #print tmp\n",
    "            return None\n",
    "    else:\n",
    "        return None   \n",
    "\n",
    "def rem_streetname(row):    \n",
    "    x = row['display_address']\n",
    "    y = row['street_address']\n",
    "    return rem_streetname_xy(x,y)\n",
    "\n",
    "\n",
    "def get_address_num_simple(x):\n",
    "    if len(x)==0:\n",
    "        return -1\n",
    "    \n",
    "    x1 = x.strip().split()[0]\n",
    "    if x1.isdigit():\n",
    "        return float(x1)\n",
    "    return -1\n",
    "\n",
    "def manhattan_locale(df1_,df2_):\n",
    "    '''\n",
    "    extract street or avenue number from displayed address. \n",
    "    \n",
    "    For those displayed address like 'w 3rd street and 5th avenue', all three fields will have values. \n",
    "    For a majority cases, one of the (west_east, street) or (avenue) will be null (0)\n",
    "    '''\n",
    "    \n",
    "    df1 = df1_.copy()\n",
    "    df2 = df2_.copy()\n",
    "    df1['source'] = 1\n",
    "    df2['source'] = 2\n",
    "    df = df1.append(df2)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    ave_mapping = {'lexington avenue': '3.5 avenue',\n",
    "                  'park avenue':'4 avenue',\n",
    "                  'madison avenue':'4.5 avenue',\n",
    "                  'central park west':'8 avenue',\n",
    "                  'columbus avenue':'9 avenue',\n",
    "                  'amsterdam avenue':'10 avenue',\n",
    "                  'west end avenue':'11 avenue'\n",
    "                  }\n",
    "    addr_adjust = {'central park west':5000,\n",
    "                  'columbus avenue':5000,\n",
    "                  'amsterdam avenue':5000,\n",
    "                  'west end avenue':5000        \n",
    "                }       \n",
    "    \n",
    "    df['west_east'] = 0 #west = -1,east = 1, null=0\n",
    "    df['street_num'] = 0 # street number e.g. 12nd street will be 12, if not on a street, empty\n",
    "    df['ave_num'] = 0 #avenue number, if not on avenue, empty\n",
    "    df['addr_num_adjust'] = 0 # for several avenues \n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "    \n",
    "    for inx,row in df.iterrows():\n",
    "        addr_str = ''\n",
    "        addr_ave = ''\n",
    "        \n",
    "        if ' and ' in row['display_address']:\n",
    "            ss = row['display_address'].split(' and ')\n",
    "            if len(ss)>2:\n",
    "                continue\n",
    "            if ' street' in ss[0] and ' avenue' in ss[1]:\n",
    "                addr_str = ss[0].strip()\n",
    "                addr_ave = ss[1].strip()\n",
    "            elif ' street' in ss[1] and ' avenue' in ss[0]:\n",
    "                addr_str = ss[1].strip()\n",
    "                addr_str = ss[0].strip()\n",
    "            else:\n",
    "                continue\n",
    "        else:            \n",
    "            if ' street' in row['display_address']:\n",
    "                addr_str = row['display_address'].strip()\n",
    "            if ' avenue' in row['display_address']:\n",
    "                addr_ave = row['display_address'].strip()\n",
    "        \n",
    "        if len(addr_str)>0:\n",
    "            num = re.sub('\\D+','',row['display_address'])\n",
    "            if len(num)>0:\n",
    "                fields = addr_str.split()\n",
    "                west_east = 0\n",
    "                ii=0\n",
    "                while ii < len(fields):\n",
    "                    if fields[ii].strip() == 'west':\n",
    "                        west_east = -1\n",
    "                        break\n",
    "                    elif fields[ii].strip() == 'east':\n",
    "                        west_east = 1\n",
    "                        break\n",
    "                    ii += 1\n",
    "                if ii<len(fields)-2:                    \n",
    "                    df.set_value(inx,'west_east',west_east)   \n",
    "                    try:\n",
    "                        df.set_value(inx,'street_num',float(re.sub('\\D+','',fields[ii+1])))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                        #print addr_str\n",
    "                        #return None,None\n",
    "        if len(addr_ave)>0:\n",
    "            adjust = 0\n",
    "            if addr_ave in addr_adjust:\n",
    "                adjust = addr_adjust[addr_ave]\n",
    "            if addr_ave in ave_mapping:\n",
    "                addr_ave = ave_mapping[addr_ave]\n",
    "            try:\n",
    "                df.set_value(inx,'ave_num',float(re.sub('\\D+','',addr_ave.split()[0])))\n",
    "            except ValueError:\n",
    "                #print addr_ave\n",
    "                pass\n",
    "            df.set_value(inx,'addr_num_adjust',adjust)\n",
    "    df['address_num'] = df['address_num'] + df['addr_num_adjust']    \n",
    "    del df['addr_num_adjust']\n",
    "    df1 = df[df['source']==1].copy()\n",
    "    df2 = df[df['source']==2].copy()\n",
    "    del df1['source']\n",
    "    del df2['source']\n",
    "    del df\n",
    "    \n",
    "    fea_list = ['west_east','street_num','ave_num']\n",
    "    return df1,df2,fea_list\n",
    "    \n",
    "\n",
    "def address_proc(train_df_,test_df_):        \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    train_df['address_num'] = train_df['street_address'].map(get_address_num_simple)\n",
    "    test_df['address_num'] = test_df['street_address'].map(get_address_num_simple)\n",
    "    train_df['display_address'] = train_df['display_address'].map(lambda x:normalize_street(x))\n",
    "    test_df['display_address'] = test_df['display_address'].map(lambda x:normalize_street(x))\n",
    "    train_df['street_address'] = train_df['street_address'].map(lambda x:normalize_street(x))\n",
    "    test_df['street_address'] = test_df['street_address'].map(lambda x:normalize_street(x))\n",
    "    \n",
    "    flist = ['address_num']\n",
    "    train_df,test_df,ff1 = manhattan_locale(train_df,test_df)\n",
    "    #train_df,test_df,ff2 = multiple_hashing(train_df,test_df,'display_address')\n",
    "    #train_df,test_df,ff2 = simple_hashing(train_df,test_df,'street_address')\n",
    "    \n",
    "    flist = flist + ff1\n",
    "    return train_df,test_df,flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# price quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_quantile_by_key(df,keys,nLevel,newcol_name):\n",
    "    #keys is a list of keys to groupby on\n",
    "    gp = df.groupby(keys)\n",
    "    levels = np.arange(0,1,1./nLevel)\n",
    "    res = pd.DataFrame()\n",
    "    \n",
    "    for inx,data in gp:\n",
    "        quantiles = [data['price'].quantile(x) for x in levels]\n",
    "        #import pdb;pdb.set_trace()\n",
    "        tmp = data.copy()\n",
    "        tmp[newcol_name] = data['price'].map(lambda x:np.searchsorted(quantiles,x))\n",
    "        \n",
    "        res = res.append(tmp)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    df = res\n",
    "    return df\n",
    "\n",
    "def quantile_price(df1_,df2_,nLevel=10):\n",
    "    df1 = df1_.copy()\n",
    "    df2 = df2_.copy()\n",
    "    df1['source'] = 1\n",
    "    df2['source'] = 2\n",
    "    df = df1.append(df2)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = get_quantile_by_key(df,['bedrooms','bathrooms'],nLevel,'price_quantile')\n",
    "                \n",
    "    df1 = df[df['source']==1].copy()\n",
    "    df2 = df[df['source']==2].copy()\n",
    "    del df1['source']\n",
    "    del df2['source']\n",
    "    del df\n",
    "    \n",
    "    return df1,df2,['price_quantile']\n",
    "\n",
    "def quantile_price_lat_long(df1_,df2_,step_size=0.02,nLevel=10):\n",
    "    df1 = df1_.copy()\n",
    "    df2 = df2_.copy()\n",
    "    df1['source'] = 1\n",
    "    df2['source'] = 2\n",
    "    df = df1.append(df2)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    west, south, east, north = -74.02, 40.64, -73.85, 40.86\n",
    "    df['long_grid'] = df['longitude'].map(lambda x: int(round((x-west)/step_size)))\n",
    "    df['lat_grid'] = df['latitude'].map(lambda x:int(round((x-south)/step_size)))\n",
    "    \n",
    "    df = get_quantile_by_key(df,['long_grid','lat_grid','bedrooms','bathrooms'],nLevel,'price_quantile_lat_long')\n",
    "                \n",
    "    df1 = df[df['source']==1].copy()\n",
    "    df2 = df[df['source']==2].copy()\n",
    "    del df1['source']\n",
    "    del df2['source']\n",
    "    del df\n",
    "    \n",
    "    return df1,df2,['price_quantile_lat_long','long_grid','lat_grid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoricalFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_categorical(self,df1_,df2_):\n",
    "        df1 = df1_.copy()\n",
    "        df2 = df2_.copy()\n",
    "        df1['source'] = 1\n",
    "        df2['source'] = 2\n",
    "        df = df1.append(df2)\n",
    "        df = df.reset_index(drop=True)\n",
    "    \n",
    "    #some of the building id is zero, whereas other rows with same street address have nonzero building_id\n",
    "    #nonzero = df[(df['building_id']!='0')&(df['street_address']!='')&(df['street_address'].notnull())]\n",
    "    #id_addr = nonzero.groupby('street_address')['building_id'].first().reset_index()\n",
    "    #del df['building_id']\n",
    "    #df = pd.merge(df,id_addr,on='street_address',how='left')\n",
    "    #df['building_id'] = df['building_id'].fillna('0')\n",
    "    \n",
    "    #assign those categorical data that only appear once to the same value\n",
    "        def objects_with_only_one_record(df,feature_name):\n",
    "            #import pdb;pdb.set_trace()\n",
    "            temp = df.groupby(feature_name, as_index = False).count()\n",
    "            return temp[temp['source'] == 1]\n",
    "\n",
    "        #import pdb;pdb.set_trace()\n",
    "        managers_with_one_lot = objects_with_only_one_record(df,'manager_id')\n",
    "        buildings_with_one_lot = objects_with_only_one_record(df,'building_id')\n",
    "        addresses_with_one_lot = objects_with_only_one_record(df,'display_address')\n",
    "\n",
    "        df.loc[df['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n",
    "          'manager_id'] = \"once\"\n",
    "        df.loc[df['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n",
    "          'building_id'] = \"once\"\n",
    "        df.loc[df['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n",
    "          'display_address'] = \"once\"\n",
    "                \n",
    "        df1 = df[df['source']==1].copy()\n",
    "        df2 = df[df['source']==2].copy()\n",
    "        del df1['source']\n",
    "        del df2['source']\n",
    "        del df\n",
    "    \n",
    "        return df1,df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class categorical_cv(CategoricalFeature):\n",
    "    def __init__(self,nfold,k=5.0,f=1.0,r_k=0.01,g=1.0):\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "        self.r_k = r_k\n",
    "        self.g = g\n",
    "        self.nfold = nfold\n",
    "        \n",
    "    def cat2num(self,df_tr_,df_te_,cat_var,target):\n",
    "        #example: cat_var = 'building_id', target='medium'\n",
    "        dftrain = df_tr_.copy()\n",
    "        dftest = df_te_.copy()\n",
    "    \n",
    "        global_avg = dftrain[target].sum()*1.0/len(dftrain)\n",
    "        gp = dftrain.groupby(cat_var)\n",
    "        cat_avg = gp[target].agg({'avg':np.mean,\n",
    "                             'cnt':lambda x:len(x)})\n",
    "        cat_avg = cat_avg.reset_index()\n",
    "        cat_avg['beta'] = cat_avg['cnt'].map(lambda x:1./(self.g+np.exp((x-self.k)/self.f)) if x<200 else 0.)\n",
    "    \n",
    "        cat_avg['cat2num'] = cat_avg['avg']*(1-cat_avg['beta']) + global_avg*cat_avg['beta']\n",
    "        dftest = pd.merge(dftest,cat_avg[[cat_var,'cat2num']],on=cat_var,how='left')\n",
    "        dftest['cat2num'] = dftest['cat2num'].fillna(global_avg)\n",
    "    \n",
    "        return dftest['cat2num'].as_matrix()\n",
    "    def categorical_average(self,Xtrain_,Xtest_,variable,target):       \n",
    "        X_train = Xtrain_.copy()\n",
    "        X_test = Xtest_.copy()\n",
    "    \n",
    "        k_fold = StratifiedKFold(self.nfold,shuffle=True,random_state=222)\n",
    "        fea_name = variable + '_' + target\n",
    "        fea_train = np.zeros(len(X_train))\n",
    "        fea_test = np.zeros(len(X_test))\n",
    "    \n",
    "        for train_inx,cv_inx in k_fold.split(np.zeros((len(X_train),2)),X_train['interest_level'].ravel()):\n",
    "            fea_train[cv_inx] = self.cat2num(X_train.iloc[train_inx,:],X_train.iloc[cv_inx,:],variable,target)\n",
    "    \n",
    "        X_train[fea_name] = fea_train\n",
    "        X_test.loc[:,fea_name] = self.cat2num(X_train,X_test,variable,target)\n",
    "        return X_train,X_test,fea_name\n",
    "    \n",
    "    def transform(self,xtrain_,xtest_):\n",
    "        Xtrain,Xtest = self.prepare_categorical(xtrain_,xtest_)\n",
    "    \n",
    "        categorical = ['building_id', 'manager_id', 'display_address']\n",
    "        fea_list = categorical\n",
    "        for f in categorical:\n",
    "            encoder = preprocessing.LabelEncoder()\n",
    "            encoder.fit(list(Xtrain[f]) + list(Xtest[f])) \n",
    "            Xtrain[f] = encoder.transform(Xtrain[f].ravel())\n",
    "            Xtest[f] = encoder.transform(Xtest[f].ravel())\n",
    "    \n",
    "        Xtrain['low'] = 0\n",
    "        Xtrain.loc[Xtrain['interest_level'] == 0, 'low'] = 1\n",
    "        Xtrain['medium'] = 0\n",
    "        Xtrain.loc[Xtrain['interest_level'] == 1, 'medium'] = 1\n",
    "        Xtrain['high'] = 0\n",
    "        Xtrain.loc[Xtrain['interest_level'] == 2, 'high'] = 1\n",
    "    \n",
    "        for col in [\"building_id\", \"manager_id\"]:        \n",
    "            Xtrain,Xtest,fea1 = self.categorical_average(Xtrain,Xtest,col, \"medium\")\n",
    "            Xtrain,Xtest,fea2 = self.categorical_average(Xtrain,Xtest,col, \"high\")\n",
    "            fea_list += [fea1,fea2]\n",
    "    \n",
    "    \n",
    "        return Xtrain,Xtest,fea_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class categorical_lit(CategoricalFeature):\n",
    "    def __init__(self,nfold,k=5.0,f=1.0,r_k=0.01,g=1.0):\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "        self.r_k = r_k\n",
    "        self.g = g\n",
    "        self.nfold = nfold\n",
    "        self.global_avg = None\n",
    "        \n",
    "    def cat2num(self,df_tr_,df_te_,cat_var,target):\n",
    "        #example: cat_var = 'building_id', target='medium'\n",
    "        dftrain = df_tr_.copy()\n",
    "        dftest = df_te_.copy()\n",
    "    \n",
    "        \n",
    "        gp = dftrain.groupby(cat_var)\n",
    "        cat_avg = gp[target].agg({'avg':np.mean,\n",
    "                             'cnt':lambda x:len(x)})\n",
    "        cat_avg = cat_avg.reset_index()\n",
    "        cat_avg['beta'] = cat_avg['cnt'].map(lambda x:1./(self.g+np.exp((x-self.k)/self.f)) if x<200 else 0.)\n",
    "    \n",
    "        cat_avg['cat2num'] = cat_avg['avg']*(1-cat_avg['beta']) + self.global_avg*cat_avg['beta']\n",
    "        \n",
    "        dftest = pd.merge(dftest,cat_avg[[cat_var,'cat2num']],on=cat_var,how='left')\n",
    "        dftest['cat2num'] = dftest['cat2num'].fillna(self.global_avg)\n",
    "        if self.r_k: \n",
    "            ratio = np.random.uniform(1 - self.r_k, 1 + self.r_k, len(dftest))\n",
    "        else:\n",
    "            ratio = 1\n",
    "        return dftest['cat2num'].as_matrix()*ratio\n",
    "    \n",
    "    def categorical_average(self,Xtrain_,Xtest_,variable,target):       \n",
    "        X_train = Xtrain_.copy()\n",
    "        X_test = Xtest_.copy()\n",
    "    \n",
    "        self.global_avg = X_train[target].sum()*1.0/len(X_train)\n",
    "        \n",
    "        k_fold = StratifiedKFold(self.nfold,shuffle=True,random_state=222)\n",
    "        fea_name = variable + '_' + target +'_lit'\n",
    "        fea_train = np.zeros(len(X_train))\n",
    "        fea_test = np.zeros(len(X_test))\n",
    "    \n",
    "        for train_inx,cv_inx in k_fold.split(np.zeros((len(X_train),2)),X_train['interest_level'].ravel()):\n",
    "            fea_train[cv_inx] = self.cat2num(X_train.iloc[train_inx,:],X_train.iloc[cv_inx,:],variable,target)\n",
    "    \n",
    "        X_train[fea_name] = fea_train\n",
    "        X_test.loc[:,fea_name] = self.cat2num(X_train,X_test,variable,target)\n",
    "        return X_train,X_test,fea_name\n",
    "    \n",
    "    def transform(self,xtrain_,xtest_):\n",
    "        Xtrain,Xtest = self.prepare_categorical(xtrain_,xtest_)\n",
    "    \n",
    "        categorical = ['building_id', 'manager_id', 'display_address']\n",
    "        fea_list = categorical\n",
    "        for f in categorical:\n",
    "            encoder = preprocessing.LabelEncoder()\n",
    "            encoder.fit(list(Xtrain[f]) + list(Xtest[f])) \n",
    "            Xtrain[f] = encoder.transform(Xtrain[f].ravel())\n",
    "            Xtest[f] = encoder.transform(Xtest[f].ravel())\n",
    "    \n",
    "        Xtrain['low'] = 0\n",
    "        Xtrain.loc[Xtrain['interest_level'] == 0, 'low'] = 1\n",
    "        Xtrain['medium'] = 0\n",
    "        Xtrain.loc[Xtrain['interest_level'] == 1, 'medium'] = 1\n",
    "        Xtrain['high'] = 0\n",
    "        Xtrain.loc[Xtrain['interest_level'] == 2, 'high'] = 1\n",
    "    \n",
    "        for col in [\"building_id\", \"manager_id\"]:        \n",
    "            Xtrain,Xtest,fea1 = self.categorical_average(Xtrain,Xtest,col, \"medium\")\n",
    "            Xtrain,Xtest,fea2 = self.categorical_average(Xtrain,Xtest,col, \"high\")\n",
    "            fea_list += [fea1,fea2]\n",
    "    \n",
    "    \n",
    "        return Xtrain,Xtest,fea_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getQuant(df1_,df2_,independent_var,output_name):\n",
    "    #independent_var can be:\n",
    "    # 1. ['longitude','latitude']\n",
    "    # 2. ['longitude','latitude','bathrooms']\n",
    "    # 3. ['polar_rho','polar_theta']\n",
    "    # 4. ['polar_rho','polar_theta','bathrooms']\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "    df1 = df1_.copy()\n",
    "    df2 = df2_.copy()\n",
    "    df1['source'] = 1\n",
    "    df2['source'] = 2\n",
    "    df = df1.append(df2)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df[output_name] = 4\n",
    "    \n",
    "    for room in range(0,6):\n",
    "        if room>=5:\n",
    "            mask = (df['bedrooms']>=5)\n",
    "        else:\n",
    "            mask = (df['bedrooms']==room)\n",
    "        tmp = df[mask].copy()\n",
    "        \n",
    "        jj = 0\n",
    "        all_quantile = np.zeros((len(tmp),9))\n",
    "        for alpha in np.arange(0.1,1,0.1):\n",
    "            rgr = GradientBoostingRegressor(loss='quantile',alpha=alpha,n_estimators=100,max_depth=2)\n",
    "            rgr.fit(tmp[independent_var],tmp['price'].as_matrix().ravel())\n",
    "            pred = rgr.predict(tmp[independent_var])\n",
    "            all_quantile[:,jj] = pred\n",
    "            jj += 1\n",
    "        quant_res = [np.searchsorted(all_quantile[ii,:],tmp['price'].iloc[ii]) for ii in range(len(tmp))]\n",
    "        df.loc[mask,output_name] = quant_res\n",
    "        \n",
    "    \n",
    "    df1 = df[df['source']==1].copy()\n",
    "    df2 = df[df['source']==2].copy()\n",
    "    del df1['source']\n",
    "    del df2['source']\n",
    "    del df\n",
    "    \n",
    "    return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additional_feature(train_df_,test_df_):    \n",
    "    train_df = train_df_.copy()\n",
    "    test_df = test_df_.copy()\n",
    "    \n",
    "    train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "    test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "    test_df['days'] = test_df['created']-pd.to_datetime('2016-04-01')\n",
    "    train_df['days'] = train_df['created'] - pd.to_datetime('2016-04-01')\n",
    "    test_df['hours'] = test_df['days']/np.timedelta64(1,'h')\n",
    "    train_df['hours'] = train_df['days']/np.timedelta64(1,'h')\n",
    "    test_df['days'] = test_df['days']/np.timedelta64(1, 'D')\n",
    "    train_df['days'] = train_df['days']/np.timedelta64(1,'D')\n",
    "    \n",
    "    train_df['hours'] = train_df['hours'].map(int)\n",
    "    test_df['hours'] = test_df['hours'].map(int)\n",
    "    gp = train_df.append(test_df).groupby('hours').size()\n",
    "    gp.name = 'hour_size'\n",
    "    gp = gp.reset_index()\n",
    "    train_df = pd.merge(train_df,gp,on='hours')\n",
    "    test_df = pd.merge(test_df,gp,on='hours')\n",
    "    del test_df['hours']\n",
    "    del train_df['hours']\n",
    "    \n",
    "    train_df['weekdays'] = train_df['created'].map(lambda x:x.weekday())\n",
    "    test_df['weekdays'] = test_df['created'].map(lambda x:x.weekday())\n",
    "\n",
    "    # Let us extract some features like year, month, day, hour from date columns #\n",
    "    train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "    test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "    train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "    test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "    train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "    test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "    # count of photos #\n",
    "    train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "    test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "    # count of \"features\" #\n",
    "    train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "    test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "    # count of words present in description column #\n",
    "    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    \n",
    "    tmp = train_df.append(test_df)\n",
    "    gp = tmp.groupby('manager_id').size()\n",
    "    gp.name = 'manager_count'\n",
    "    gp = gp.reset_index()\n",
    "    train_df = pd.merge(train_df,gp,how='left')\n",
    "    test_df = pd.merge(test_df,gp,how='left')\n",
    "    \n",
    "    gp = tmp.groupby('building_id').size()\n",
    "    gp.name = 'building_count'\n",
    "    gp = gp.reset_index()\n",
    "    train_df = pd.merge(train_df,gp,how='left')\n",
    "    test_df = pd.merge(test_df,gp,how='left')\n",
    "    \n",
    "    #train_df['price_per_bath'] = train_df['price'] / train_df['bathrooms']\n",
    "    #train_df['price_per_room'] = train_df['price'] / (train_df['bathrooms'] + train_df['bedrooms'] )\n",
    "\n",
    "    #test_df['price_per_bath'] = test_df['price'] / test_df['bathrooms']\n",
    "    #test_df['price_per_room'] = test_df['price'] / (0.5*test_df['bathrooms'] + test_df['bedrooms'] )\n",
    "    \n",
    "    fea_list = ['weekdays','manager_count','building_count',\"num_features\",\"num_description_words\",\"days\",\"num_photos\", \"created_month\", \"created_day\", \"created_hour\"]\n",
    "    return train_df,test_df,fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data_path = \"../input/\"\n",
    "    train_file = data_path + \"train.json\"\n",
    "    test_file = data_path + \"test.json\"\n",
    "    train_df = pd.read_json(train_file)\n",
    "    test_df = pd.read_json(test_file)\n",
    "    interest_map = {'low':0,'medium':1,'high':2}\n",
    "    train_df['interest_level'] = train_df['interest_level'].map(interest_map)\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    train_df[\"street_address\"] = train_df['street_address'].apply(fmt)\n",
    "    train_df[\"display_address\"] = train_df[\"display_address\"].apply(fmt)\n",
    "    #original_col = train_df.columns\n",
    "    test_df[\"street_address\"] = test_df['street_address'].apply(fmt)\n",
    "    test_df[\"display_address\"] = test_df[\"display_address\"].apply(fmt)\n",
    "    \n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB_sklearn_calibration(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=2800,verbose=True):\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=num_rounds,\n",
    "                            objective='multi:softprob',\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=6,\n",
    "                            min_child_weight=1,\n",
    "                            subsample=.7,\n",
    "                            colsample_bytree=.7,\n",
    "                            colsample_bylevel=.5,\n",
    "                            gamma=0.005,\n",
    "                            scale_pos_weight=1,\n",
    "                            base_score=.5,\n",
    "                            #reg_lambda=0,\n",
    "                            #reg_alpha=0,\n",
    "                            #missing=0,\n",
    "                            seed=seed_val)\n",
    "    \n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    clf_isotonic = CalibratedClassifierCV(clf, cv=3, method='isotonic')\n",
    "    clf_isotonic.fit(train_X, train_y)\n",
    "    prob_iso = clf_isotonic.predict_proba(test_X)\n",
    "\n",
    "    # Gaussian Naive-Bayes with sigmoid calibration\n",
    "    clf_sigmoid = CalibratedClassifierCV(clf, cv=3, method='sigmoid')\n",
    "    clf_sigmoid.fit(train_X, train_y)\n",
    "    prob_sig = clf_sigmoid.predict_proba(test_X)\n",
    "    \n",
    "    return prob_iso,prob_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB_sklearn(train_X, train_y,  test_X, test_y=None,sample_weight=None, feature_names=None, seed_val=0, num_rounds=5000,verbose=True):\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=num_rounds,\n",
    "                            objective='multi:softprob',\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=6,\n",
    "                            min_child_weight=1,\n",
    "                            subsample=.7,\n",
    "                            colsample_bytree=.7,\n",
    "                            colsample_bylevel=.5,\n",
    "                            gamma=0.005,\n",
    "                            scale_pos_weight=1,\n",
    "                            base_score=.5,\n",
    "                            #reg_lambda=0,\n",
    "                            #reg_alpha=0,\n",
    "                            #missing=0,\n",
    "                            seed=seed_val)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        clf.fit(train_X, train_y,sample_weight= sample_weight,eval_set=[(train_X, train_y), (test_X, test_y)],verbose=verbose,eval_metric='mlogloss',\n",
    "            early_stopping_rounds=50)\n",
    "    else:        \n",
    "        clf.fit(train_X, train_y,sample_weight = sample_weight,verbose=False)\n",
    "    pred_test_y = clf.predict_proba(test_X)\n",
    "    return pred_test_y, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=3000,\n",
    "                            objective='multi:softprob',\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=6,\n",
    "                            min_child_weight=1,\n",
    "                            subsample=.7,\n",
    "                            colsample_bytree=.7,\n",
    "                            colsample_bylevel=.5,\n",
    "                            gamma=0.005,\n",
    "                            scale_pos_weight=1,\n",
    "                            base_score=.5,\n",
    "                            #reg_lambda=0,\n",
    "                            #reg_alpha=0,\n",
    "                            #missing=0,\n",
    "                            seed=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_cv(train_df,test_df):\n",
    "    train_X = train_df[fealist].as_matrix()\n",
    "    test_X = test_df[fealist].as_matrix()\n",
    "    train_y = np.array(train_df['interest_level'])\n",
    "    \n",
    "    import pdb;pdb.set_trace()\n",
    "    \n",
    "    cv_scores = [] \n",
    "    #print fea_categorical\n",
    "    #print fea_additional\n",
    "    #print feature_params\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "    for dev_index, val_index in kf.split(train_X,train_y):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB_sklearn(dev_X, dev_y, val_X, val_y,verbose=False)\n",
    "        print('best iterations:{}, best_score={}, last_score={}'.format(model.best_iteration,\n",
    "                                                                   model.best_score,log_loss(val_y, preds)))\n",
    "        importance_inx = np.argsort(model.feature_importances_*-1)\n",
    "        print('Most important 40 features:')\n",
    "        ff = [(fealist[x],model.feature_importances_[x]) for x in importance_inx[:40]]\n",
    "        print(ff)\n",
    "        print('-------------------------')\n",
    "          \n",
    "    \n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "    print 'mean score={}'.format(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Global parameter\n",
    "feature_params = {}\n",
    "feature_params['nTextFea'] = 300 # Number of text features used}\n",
    "feature_params['nQuantLevel'] = 10 # number of levels for quantile computation\n",
    "feature_params['step_size'] = 0.02 # the step size when discreting latitude and longitude\n",
    "feature_params['category_nfold'] = 5 # n-fold to transform the categorical variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df,test_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10        1\n",
       "10000     0\n",
       "100004    2\n",
       "100007    0\n",
       "100013    0\n",
       "100014    1\n",
       "100016    0\n",
       "100020    0\n",
       "100026    1\n",
       "100027    0\n",
       "Name: interest_level, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['interest_level'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fea_list length is 303\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df,test_df,fealist_txt = text_fea(train_df,test_df,nTop = feature_params['nTextFea'])\n",
    "#train_df,test_df,fealist_addr = address_proc(train_df,test_df)\n",
    "\n",
    "\n",
    "#train_df,test_df,fealist_quant1 = quantile_price_lat_long(train_df,test_df,\n",
    "#                                step_size = feature_params['step_size'],nLevel=feature_params['nQuantLevel'])\n",
    "#train_df,test_df,fealist_quant2 = quantile_price(train_df,test_df,nLevel=feature_params['nQuantLevel'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df,test_df = getQuant(train_df,test_df,['latitude','longitude'],'gbm_quant_lat_long')\n",
    "#fealist_quant_gbm = ['gbm_quant_lat_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ca = categorical_cv(feature_params['category_nfold'])\n",
    "#train_df,test_df,fea_categorical = ca.transform(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df,test_df,fea_additional = additional_feature(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>price</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>weekdays</th>\n",
       "      <th>manager_count</th>\n",
       "      <th>building_count</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_description_words</th>\n",
       "      <th>days</th>\n",
       "      <th>num_photos</th>\n",
       "      <th>created_month</th>\n",
       "      <th>created_day</th>\n",
       "      <th>created_hour</th>\n",
       "      <th>interest_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>-73.9425</td>\n",
       "      <td>3000</td>\n",
       "      <td>7211212</td>\n",
       "      <td>4</td>\n",
       "      <td>235</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>84.329444</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.7715</td>\n",
       "      <td>-73.9930</td>\n",
       "      <td>2400</td>\n",
       "      <td>7210427</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>137</td>\n",
       "      <td>11</td>\n",
       "      <td>198</td>\n",
       "      <td>84.297384</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.6678</td>\n",
       "      <td>-73.9398</td>\n",
       "      <td>3050</td>\n",
       "      <td>7211226</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>20664</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>84.329988</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>40.7436</td>\n",
       "      <td>-73.9727</td>\n",
       "      <td>6100</td>\n",
       "      <td>7210946</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>457</td>\n",
       "      <td>13</td>\n",
       "      <td>210</td>\n",
       "      <td>84.318426</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40.6910</td>\n",
       "      <td>-73.9228</td>\n",
       "      <td>2895</td>\n",
       "      <td>7210714</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>84.309664</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathrooms  bedrooms  latitude  longitude  price  listing_id  weekdays  \\\n",
       "0        1.5         3   40.7145   -73.9425   3000     7211212         4   \n",
       "1        1.0         0   40.7715   -73.9930   2400     7210427         4   \n",
       "2        2.0         3   40.6678   -73.9398   3050     7211226         4   \n",
       "3        2.0         4   40.7436   -73.9727   6100     7210946         4   \n",
       "4        1.0         3   40.6910   -73.9228   2895     7210714         4   \n",
       "\n",
       "   manager_count  building_count  num_features  num_description_words  \\\n",
       "0            235               5             0                     95   \n",
       "1             38             137            11                    198   \n",
       "2             55           20664             8                    183   \n",
       "3             38             457            13                    210   \n",
       "4              7               1             4                    122   \n",
       "\n",
       "        days  num_photos  created_month  created_day  created_hour  \\\n",
       "0  84.329444           5              6           24             7   \n",
       "1  84.297384           7              6           24             7   \n",
       "2  84.329988           8              6           24             7   \n",
       "3  84.318426           8              6           24             7   \n",
       "4  84.309664           8              6           24             7   \n",
       "\n",
       "   interest_level  \n",
       "0               1  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[fealist+['interest_level']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fealist = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",'listing_id'] \n",
    "fealist = fealist + fea_additional\n",
    "#fealist = fealist+fealist_txt+fealist_addr +fea_additional\n",
    "#fealist = fealist+ fealist_quant1 + fealist_quant2+fealist_txt+fealist_addr + fea_additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fealist_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(a,b,c):\n",
    "    print a+b\n",
    "    print a+c\n",
    "    \n",
    "class foo():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def ff(self,x,**arg):\n",
    "        print x*2\n",
    "        func(**arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "tmp.ff(1,a=4,b=5,c=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "5    0.0\n",
       "6    0.0\n",
       "7    0.0\n",
       "8    0.0\n",
       "9    0.0\n",
       "Name: interest_level, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['interest_level'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(tmp, 'predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
